{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import gensim\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 1\n",
    "def init_random_seed(value=0):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "init_random_seed(SEED)\n",
    "    \n",
    "pd.set_option('display.max_colwidth', 255)\n",
    "tqdm.pandas()\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=8, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Model, T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = pd.read_csv(\"../data/abbr.csv\")\n",
    "lenta_train = pd.read_csv(\"../data/lenta_train.csv\")\n",
    "lenta_test = pd.read_csv(\"../data/lenta_test.csv\")\n",
    "\n",
    "def get_abbr(abbr_id):\n",
    "    return abbr[abbr[\"abbr_id\"] == abbr_id][\"abbr_norm\"].iloc[0]\n",
    "\n",
    "def get_desc(abbr_id):\n",
    "    return abbr[abbr[\"abbr_id\"] == abbr_id][\"desc_norm\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fill_task(text, labels, window_size=10):\n",
    "    text = text.copy()\n",
    "    abbr_id = random.choices(list(set(label) - set(\"_\")))[0]\n",
    "    ind = labels.index(abbr_id)\n",
    "    abbr_id = int(abbr_id.replace(\"W-\", \"\"))\n",
    "    desc = get_desc(abbr_id)\n",
    "    abbr_norm = text[ind]\n",
    "    text[ind] = \"<extra_id_1>\"\n",
    "    l = max(0, ind - window_size)\n",
    "    r = ind + window_size\n",
    "    pair = f\"fill {abbr_norm} | {' '.join(text[l:r])}\" \n",
    "    return pair, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 548700/548700 [01:05<00:00, 8336.99it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = lenta_train[\"text_new\"].to_list()\n",
    "labels = lenta_train[\"labels_new\"].to_list()\n",
    "\n",
    "pairs = []\n",
    "for i in tqdm(range(len(texts))):\n",
    "    text = texts[i].split()\n",
    "    label = labels[i].split()\n",
    "    \n",
    "    try:\n",
    "        pairs.append(get_fill_task(text, label))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fill кг | обладминистрация в сб 20 фев подарочный набор состоять из семь <extra_id_1> куриный мясо пять кило гречневый крупа и шесть литр',\n",
       " 'килограмм')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 222903552\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "MODEL_NAME = \"sberbank-ai/ruT5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, **kwargs):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        hypotheses = model.generate(**inputs, **kwargs)\n",
    "    return tokenizer.decode(hypotheses[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709db1ff1bdd488ca71084b5f7404978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 11.86892032623291\n",
      "step 20 loss 9.378391814231872\n",
      "step 40 loss 6.49945182800293\n",
      "step 60 loss 5.952630472183228\n",
      "step 80 loss 5.5946629524230955\n",
      "step 100 loss 5.101913475990296\n",
      "step 120 loss 4.919951748847962\n",
      "step 140 loss 4.662925553321839\n",
      "step 160 loss 4.4793765306472775\n",
      "step 180 loss 4.487955963611602\n",
      "step 200 loss 4.281306004524231\n",
      "step 220 loss 4.0929787993431095\n",
      "step 240 loss 4.193723571300507\n",
      "step 260 loss 4.217689514160156\n",
      "step 280 loss 3.888678765296936\n",
      "step 300 loss 3.7140239000320436\n",
      "step 320 loss 3.9411176800727845\n",
      "step 340 loss 3.798016381263733\n",
      "step 360 loss 3.612498414516449\n",
      "step 380 loss 3.5130589485168455\n",
      "step 400 loss 3.6169383049011232\n",
      "step 420 loss 3.33280154466629\n",
      "step 440 loss 3.6183501482009888\n",
      "step 460 loss 3.442434549331665\n",
      "step 480 loss 3.36221821308136\n",
      "step 500 loss 3.151871645450592\n",
      "step 520 loss 3.210953783988953\n",
      "step 540 loss 3.0059702515602114\n",
      "step 560 loss 3.116533374786377\n",
      "step 580 loss 3.090851294994354\n",
      "step 600 loss 3.1066410541534424\n",
      "step 620 loss 2.9316922307014464\n",
      "step 640 loss 2.9055022239685058\n",
      "step 660 loss 2.799181890487671\n",
      "step 680 loss 2.6766764521598816\n",
      "step 700 loss 2.702897983789444\n",
      "step 720 loss 2.6364989995956423\n",
      "step 740 loss 2.8620014250278474\n",
      "step 760 loss 2.6393154084682466\n",
      "step 780 loss 2.749145197868347\n",
      "step 800 loss 2.531703841686249\n",
      "step 820 loss 2.5746449708938597\n",
      "step 840 loss 2.504885125160217\n",
      "step 860 loss 2.5385182976722716\n",
      "step 880 loss 2.5004015147686003\n",
      "step 900 loss 2.5499060273170473\n",
      "step 920 loss 2.5315166234970095\n",
      "step 940 loss 2.472872292995453\n",
      "step 960 loss 2.229459983110428\n",
      "step 980 loss 2.523186129331589\n",
      "step 1000 loss 2.4388207256793977\n",
      "step 1020 loss 2.289169412851334\n",
      "step 1040 loss 2.408051735162735\n",
      "step 1060 loss 2.212999254465103\n",
      "step 1080 loss 2.2664584517478943\n",
      "step 1100 loss 2.447797065973282\n",
      "step 1120 loss 2.032548350095749\n",
      "step 1140 loss 2.145629197359085\n",
      "step 1160 loss 2.1117735326290132\n",
      "step 1180 loss 2.179705935716629\n",
      "step 1200 loss 2.1456314980983735\n",
      "step 1220 loss 2.040363872051239\n",
      "step 1240 loss 1.9381813943386077\n",
      "step 1260 loss 1.968920010328293\n",
      "step 1280 loss 1.9042114436626434\n",
      "step 1300 loss 2.119351440668106\n",
      "step 1320 loss 1.9047329127788544\n",
      "step 1340 loss 1.9116383075714112\n",
      "step 1360 loss 2.074943393468857\n",
      "step 1380 loss 1.9788567841053009\n",
      "step 1400 loss 1.87016162276268\n",
      "step 1420 loss 1.8943138688802719\n",
      "step 1440 loss 1.8718188643455504\n",
      "step 1460 loss 1.7891678750514983\n",
      "step 1480 loss 1.8714377522468566\n",
      "step 1500 loss 1.7722252547740935\n",
      "step 1520 loss 1.7580035299062728\n",
      "step 1540 loss 1.6536042273044587\n",
      "step 1560 loss 1.9423356294631957\n",
      "step 1580 loss 1.6479894757270812\n",
      "step 1600 loss 1.6541023313999177\n",
      "step 1620 loss 1.8753280997276307\n",
      "step 1640 loss 1.8317916452884675\n",
      "step 1660 loss 1.742639273405075\n",
      "step 1680 loss 1.7060611724853516\n",
      "step 1700 loss 1.5659177482128144\n",
      "step 1720 loss 1.8274036258459092\n",
      "step 1740 loss 1.6785058557987214\n",
      "step 1760 loss 1.6350513815879821\n",
      "step 1780 loss 1.663970908522606\n",
      "step 1800 loss 1.5920010149478911\n",
      "step 1820 loss 1.6446005940437316\n",
      "step 1840 loss 1.5787569850683212\n",
      "step 1860 loss 1.5863591998815536\n",
      "step 1880 loss 1.560364469885826\n",
      "step 1900 loss 1.5453253030776977\n",
      "step 1920 loss 1.5398112207651138\n",
      "step 1940 loss 1.4736556708812714\n",
      "step 1960 loss 1.695110085606575\n",
      "step 1980 loss 1.565816381573677\n",
      "step 2000 loss 1.6655206739902497\n",
      "step 2020 loss 1.324458611011505\n",
      "step 2040 loss 1.5485447555780412\n",
      "step 2060 loss 1.5621185958385468\n",
      "step 2080 loss 1.368600843846798\n",
      "step 2100 loss 1.336952020227909\n",
      "step 2120 loss 1.4661819756031036\n",
      "step 2140 loss 1.4377572804689407\n",
      "step 2160 loss 1.4234941929578782\n",
      "step 2180 loss 1.3679659366607666\n",
      "step 2200 loss 1.4740416288375855\n",
      "step 2220 loss 1.3341703057289123\n",
      "step 2240 loss 1.4858950972557068\n",
      "step 2260 loss 1.4536961197853089\n",
      "step 2280 loss 1.4086262613534928\n",
      "step 2300 loss 1.253857061266899\n",
      "step 2320 loss 1.3702664494514465\n",
      "step 2340 loss 1.5623873054981232\n",
      "step 2360 loss 1.517992413043976\n",
      "step 2380 loss 1.298916494846344\n",
      "step 2400 loss 1.2566389113664627\n",
      "step 2420 loss 1.1326210916042327\n",
      "step 2440 loss 1.4091143369674684\n",
      "step 2460 loss 1.4366507083177567\n",
      "step 2480 loss 1.1740492820739745\n",
      "step 2500 loss 1.2728973209857941\n",
      "step 2520 loss 1.2341397523880004\n",
      "step 2540 loss 1.347335296869278\n",
      "step 2560 loss 1.3405481964349746\n",
      "step 2580 loss 1.1572631761431693\n",
      "step 2600 loss 1.3961939722299577\n",
      "step 2620 loss 1.2152655601501465\n",
      "step 2640 loss 1.3871631413698196\n",
      "step 2660 loss 1.2171900153160096\n",
      "step 2680 loss 1.1511024922132491\n",
      "step 2700 loss 1.157449448108673\n",
      "step 2720 loss 1.3085037618875504\n",
      "step 2740 loss 1.306336584687233\n",
      "step 2760 loss 1.263204462826252\n",
      "step 2780 loss 1.2361697807908059\n",
      "step 2800 loss 1.088648146390915\n",
      "step 2820 loss 1.2606860131025315\n",
      "step 2840 loss 1.1809233173727989\n",
      "step 2860 loss 1.3201924443244935\n",
      "step 2880 loss 1.193128329515457\n",
      "step 2900 loss 1.3619832515716552\n",
      "step 2920 loss 1.2111691206693649\n",
      "step 2940 loss 1.3554862529039382\n",
      "step 2960 loss 1.076259544491768\n",
      "step 2980 loss 1.1356486707925797\n",
      "step 3000 loss 1.0308340042829514\n",
      "step 3020 loss 1.269721221923828\n",
      "step 3040 loss 1.1816510140895844\n",
      "step 3060 loss 0.9098402366042138\n",
      "step 3080 loss 1.1322282940149306\n",
      "step 3100 loss 1.2086761713027954\n",
      "step 3120 loss 1.0500292509794236\n",
      "step 3140 loss 1.0601116880774497\n",
      "step 3160 loss 1.2007120668888092\n",
      "step 3180 loss 1.2018708869814874\n",
      "step 3200 loss 1.2070704609155656\n",
      "step 3220 loss 1.346634641289711\n",
      "step 3240 loss 1.1687517791986466\n",
      "step 3260 loss 1.0183005839586259\n",
      "step 3280 loss 1.1162686079740525\n",
      "step 3300 loss 0.971835745871067\n",
      "step 3320 loss 1.1758673429489135\n",
      "step 3340 loss 1.1367438688874245\n",
      "step 3360 loss 1.0487425446510314\n",
      "step 3380 loss 1.055480483174324\n",
      "step 3400 loss 1.2510046139359474\n",
      "step 3420 loss 1.2173496156930923\n",
      "step 3440 loss 1.1363527119159698\n",
      "step 3460 loss 0.971968412399292\n",
      "step 3480 loss 1.1223954409360886\n",
      "step 3500 loss 1.0805259674787522\n",
      "step 3520 loss 1.0100779190659523\n",
      "step 3540 loss 0.9543043166399002\n",
      "step 3560 loss 1.0848914548754691\n",
      "step 3580 loss 1.034002310037613\n",
      "step 3600 loss 1.0141654402017592\n",
      "step 3620 loss 1.0944704815745354\n",
      "step 3640 loss 0.9375606372952461\n",
      "step 3660 loss 0.9840478636324406\n",
      "step 3680 loss 1.058391559123993\n",
      "step 3700 loss 1.0814971074461937\n",
      "step 3720 loss 0.9889226794242859\n",
      "step 3740 loss 1.0315934240818023\n",
      "step 3760 loss 1.0096023380756378\n",
      "step 3780 loss 1.1418278396129609\n",
      "step 3800 loss 0.892166443169117\n",
      "step 3820 loss 1.078089179098606\n",
      "step 3840 loss 0.9177410595119\n",
      "step 3860 loss 1.1100335910916328\n",
      "step 3880 loss 0.8936930075287819\n",
      "step 3900 loss 1.0617258250713348\n",
      "step 3920 loss 0.9460783451795578\n",
      "step 3940 loss 0.9805395662784576\n",
      "step 3960 loss 0.9992410391569138\n",
      "step 3980 loss 0.9964666873216629\n",
      "step 4000 loss 0.9706273198127746\n",
      "step 4020 loss 0.9682406112551689\n",
      "step 4040 loss 0.838944011926651\n",
      "step 4060 loss 1.0625074476003646\n",
      "step 4080 loss 1.0570628389716148\n",
      "step 4100 loss 1.020905989408493\n",
      "step 4120 loss 0.9593031674623489\n",
      "step 4140 loss 0.858232370018959\n",
      "step 4160 loss 0.9508917689323425\n",
      "step 4180 loss 0.9406909450888634\n",
      "step 4200 loss 0.9580008864402771\n",
      "step 4220 loss 0.9205890581011772\n",
      "step 4240 loss 1.003550261259079\n",
      "step 4260 loss 0.9412854701280594\n",
      "step 4280 loss 0.7947197079658508\n",
      "step 4300 loss 0.9418600693345069\n",
      "step 4320 loss 1.02104282528162\n",
      "step 4340 loss 0.9553793326020241\n",
      "step 4360 loss 1.0269859239459038\n",
      "step 4380 loss 1.169261634349823\n",
      "step 4400 loss 0.858688636124134\n",
      "step 4420 loss 0.988657408952713\n",
      "step 4440 loss 0.846933288872242\n",
      "step 4460 loss 0.9750794798135758\n",
      "step 4480 loss 0.9334085203707219\n",
      "step 4500 loss 0.9150076158344745\n",
      "step 4520 loss 0.8035418651998043\n",
      "step 4540 loss 0.8180838197469711\n",
      "step 4560 loss 0.8607674747705459\n",
      "step 4580 loss 0.9111480042338371\n",
      "step 4600 loss 0.8783938109874725\n",
      "step 4620 loss 0.880984902381897\n",
      "step 4640 loss 0.9290049374103546\n",
      "step 4660 loss 0.8986606322228908\n",
      "step 4680 loss 0.7809248015284538\n",
      "step 4700 loss 0.7334922611713409\n",
      "step 4720 loss 0.7939690619707107\n",
      "step 4740 loss 0.8008422628045082\n",
      "step 4760 loss 0.8688916087150573\n",
      "step 4780 loss 0.8612414196133613\n",
      "step 4800 loss 0.8212893515825271\n",
      "step 4820 loss 0.9802341476082802\n",
      "step 4840 loss 0.822496198117733\n",
      "step 4860 loss 0.928331296145916\n",
      "step 4880 loss 0.8153437353670597\n",
      "step 4900 loss 0.6097482852637768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4920 loss 0.8062367469072342\n",
      "step 4940 loss 0.9005944952368736\n",
      "step 4960 loss 0.87210054397583\n",
      "step 4980 loss 0.8682508289813995\n",
      "step 5000 loss 1.0000765338540076\n",
      "step 5020 loss 0.7942536115646363\n",
      "step 5040 loss 0.768225172907114\n",
      "step 5060 loss 0.6879331141710281\n",
      "step 5080 loss 0.6627293922007084\n",
      "step 5100 loss 0.870412340760231\n",
      "step 5120 loss 0.691646384447813\n",
      "step 5140 loss 0.7494843900203705\n",
      "step 5160 loss 0.9107828244566918\n",
      "step 5180 loss 0.7551818184554577\n",
      "step 5200 loss 0.8259865745902062\n",
      "step 5220 loss 0.8284564286470413\n",
      "step 5240 loss 0.8949624091386795\n",
      "step 5260 loss 0.7425629980862141\n",
      "step 5280 loss 0.7518613427877426\n",
      "step 5300 loss 0.7725245520472527\n",
      "step 5320 loss 0.7499701797962188\n",
      "step 5340 loss 0.8532861918210983\n",
      "step 5360 loss 0.7140013247728347\n",
      "step 5380 loss 0.7850799322128296\n",
      "step 5400 loss 0.6618482083082199\n",
      "step 5420 loss 0.9246819466352463\n",
      "step 5440 loss 0.76730320379138\n",
      "step 5460 loss 0.6968156792223453\n",
      "step 5480 loss 0.7334330350160598\n",
      "step 5500 loss 0.7264855451881885\n",
      "step 5520 loss 0.6507016964256763\n",
      "step 5540 loss 0.7593579784035682\n",
      "step 5560 loss 0.837070320546627\n",
      "step 5580 loss 0.602872222661972\n",
      "step 5600 loss 0.6620228283107281\n",
      "step 5620 loss 0.7946572363376617\n",
      "step 5640 loss 0.6736426405608654\n",
      "step 5660 loss 0.7095169372856617\n",
      "step 5680 loss 0.7947721809148789\n",
      "step 5700 loss 0.7208657190203667\n",
      "step 5720 loss 0.8776996687054635\n",
      "step 5740 loss 0.7045739777386189\n",
      "step 5760 loss 0.81929966583848\n",
      "step 5780 loss 0.6936853889375925\n",
      "step 5800 loss 0.6909695342183113\n",
      "step 5820 loss 0.8218800961971283\n",
      "step 5840 loss 0.8307867176830769\n",
      "step 5860 loss 0.771141717210412\n",
      "step 5880 loss 0.7954995438456536\n",
      "step 5900 loss 0.8485572233796119\n",
      "step 5920 loss 0.8383594579994679\n",
      "step 5940 loss 0.7640008628368378\n",
      "step 5960 loss 0.7789687000215053\n",
      "step 5980 loss 0.7771549351513386\n",
      "step 6000 loss 0.7021789815276861\n",
      "step 6020 loss 0.6800247624516487\n",
      "step 6040 loss 0.6480683378875256\n",
      "step 6060 loss 0.787124003469944\n",
      "step 6080 loss 0.711065822839737\n",
      "step 6100 loss 0.7159014917910099\n",
      "step 6120 loss 0.8088683754205703\n",
      "step 6140 loss 0.6838999718427659\n",
      "step 6160 loss 0.6910637654364109\n",
      "step 6180 loss 0.5775400266051293\n",
      "step 6200 loss 0.6833480916917324\n",
      "step 6220 loss 0.8573418259620667\n",
      "step 6240 loss 0.6895709194242954\n",
      "step 6260 loss 0.7813520956784487\n",
      "step 6280 loss 0.45475439019501207\n",
      "step 6300 loss 0.7657754266634583\n",
      "step 6320 loss 0.7126693315804005\n",
      "step 6340 loss 0.7110401537269354\n",
      "step 6360 loss 0.8282242700457573\n",
      "step 6380 loss 0.7973637154325843\n",
      "step 6400 loss 0.6650009334087372\n",
      "step 6420 loss 0.6706041924655437\n",
      "step 6440 loss 0.6145805478096008\n",
      "step 6460 loss 0.7055058628320694\n",
      "step 6480 loss 0.7128728121519089\n",
      "step 6500 loss 0.6967220041900873\n",
      "step 6520 loss 0.7709703542292118\n",
      "step 6540 loss 0.714528851956129\n",
      "step 6560 loss 0.7098523575812579\n",
      "step 6580 loss 0.5917268829420209\n",
      "step 6600 loss 0.6535682529211044\n",
      "step 6620 loss 0.6236848726868629\n",
      "step 6640 loss 0.6392467550933361\n",
      "step 6660 loss 0.7344016082584858\n",
      "step 6680 loss 0.6725041069090366\n",
      "step 6700 loss 0.6055692143738269\n",
      "step 6720 loss 0.7641112588346004\n",
      "step 6740 loss 0.8205046012997628\n",
      "step 6760 loss 0.6215122833847999\n",
      "step 6780 loss 0.8051794022321701\n",
      "step 6800 loss 0.7090021222829819\n",
      "step 6820 loss 0.6771489843726158\n",
      "step 6840 loss 0.5653738863766193\n",
      "step 6860 loss 0.6293667197227478\n",
      "step 6880 loss 0.6183681532740593\n",
      "step 6900 loss 0.7025447472929954\n",
      "step 6920 loss 0.6198471188545227\n",
      "step 6940 loss 0.7185615643858909\n",
      "step 6960 loss 0.6219615414738655\n",
      "step 6980 loss 0.6425826966762542\n",
      "step 7000 loss 0.659079073369503\n",
      "step 7020 loss 0.706498721987009\n",
      "step 7040 loss 0.63241500928998\n",
      "step 7060 loss 0.6387375313788652\n",
      "step 7080 loss 0.7380108252167702\n",
      "step 7100 loss 0.5481354337185621\n",
      "step 7120 loss 0.8015043903142214\n",
      "step 7140 loss 0.6023382917046547\n",
      "step 7160 loss 0.6574285306036473\n",
      "step 7180 loss 0.6331326015293598\n",
      "step 7200 loss 0.5710674472153187\n",
      "step 7220 loss 0.6556815221905709\n",
      "step 7240 loss 0.6731375865638256\n",
      "step 7260 loss 0.6697297926992178\n",
      "step 7280 loss 0.6074948981404305\n",
      "step 7300 loss 0.6240416504442692\n",
      "step 7320 loss 0.5598858255892992\n",
      "step 7340 loss 0.5125192128121853\n",
      "step 7360 loss 0.6206593845039606\n",
      "step 7380 loss 0.5849590033292771\n",
      "step 7400 loss 0.5709317720495164\n",
      "step 7420 loss 0.6072289321571589\n",
      "step 7440 loss 0.5078520998358727\n",
      "step 7460 loss 0.5794596128165722\n",
      "step 7480 loss 0.6613016456365586\n",
      "step 7500 loss 0.7225225575268268\n",
      "step 7520 loss 0.5279992539435625\n",
      "step 7540 loss 0.6569990448653698\n",
      "step 7560 loss 0.6433478266000747\n",
      "step 7580 loss 0.7540160378441214\n",
      "step 7600 loss 0.5967817343771458\n",
      "step 7620 loss 0.6824805796146393\n",
      "step 7640 loss 0.584879069775343\n",
      "step 7660 loss 0.5898323208093643\n",
      "step 7680 loss 0.6446781888604164\n",
      "step 7700 loss 0.5971505757421255\n",
      "step 7720 loss 0.5028147161006927\n",
      "step 7740 loss 0.6387884210795164\n",
      "step 7760 loss 0.5705938618630171\n",
      "step 7780 loss 0.5841461680829525\n",
      "step 7800 loss 0.6571889471262693\n",
      "step 7820 loss 0.5124414157122373\n",
      "step 7840 loss 0.43045982979238034\n",
      "step 7860 loss 0.6303434886038304\n",
      "step 7880 loss 0.5019534006714821\n",
      "step 7900 loss 0.5818442821502685\n",
      "step 7920 loss 0.6141680561006069\n",
      "step 7940 loss 0.5136657383292913\n",
      "step 7960 loss 0.5176751971244812\n",
      "step 7980 loss 0.5767634399235249\n",
      "step 8000 loss 0.5046780504286289\n",
      "step 8020 loss 0.5379943668842315\n",
      "step 8040 loss 0.4723584670573473\n",
      "step 8060 loss 0.5321511544287205\n",
      "step 8080 loss 0.6055328745394946\n",
      "step 8100 loss 0.5598904766142369\n",
      "step 8120 loss 0.6839441500604153\n",
      "step 8140 loss 0.6218105293810368\n",
      "step 8160 loss 0.48116344660520555\n",
      "step 8180 loss 0.5543522791005671\n",
      "step 8200 loss 0.6764943648129702\n",
      "step 8220 loss 0.5503187116235495\n",
      "step 8240 loss 0.6190776541829109\n",
      "step 8260 loss 0.45704675316810606\n",
      "step 8280 loss 0.5646674264222383\n",
      "step 8300 loss 0.6365990959107876\n",
      "step 8320 loss 0.5862586744129658\n",
      "step 8340 loss 0.6044951729476452\n",
      "step 8360 loss 0.4976749953813851\n",
      "step 8380 loss 0.5455585911870002\n",
      "step 8400 loss 0.6261012963950634\n",
      "step 8420 loss 0.645424185693264\n",
      "step 8440 loss 0.704692792519927\n",
      "step 8460 loss 0.5534591309726238\n",
      "step 8480 loss 0.6154272608458996\n",
      "step 8500 loss 0.7071574226021766\n",
      "step 8520 loss 0.5050840243697167\n",
      "step 8540 loss 0.5900354094803333\n",
      "step 8560 loss 0.5642195135354996\n",
      "step 8580 loss 0.52211057767272\n",
      "step 8600 loss 0.5200699526816607\n",
      "step 8620 loss 0.4219858482480049\n",
      "step 8640 loss 0.5136251326650381\n",
      "step 8660 loss 0.5447270166128874\n",
      "step 8680 loss 0.4380316295661032\n",
      "step 8700 loss 0.6473185360431671\n",
      "step 8720 loss 0.47694393768906596\n",
      "step 8740 loss 0.5435663461685181\n",
      "step 8760 loss 0.5692448854446411\n",
      "step 8780 loss 0.4723877986893058\n",
      "step 8800 loss 0.5951419956982136\n",
      "step 8820 loss 0.4771860459819436\n",
      "step 8840 loss 0.5888380363583565\n",
      "step 8860 loss 0.6044674031436443\n",
      "step 8880 loss 0.5387722976505757\n",
      "step 8900 loss 0.5562562815845012\n",
      "step 8920 loss 0.3923583697527647\n",
      "step 8940 loss 0.4345322761684656\n",
      "step 8960 loss 0.4746163789182901\n",
      "step 8980 loss 0.5632688019424676\n",
      "step 9000 loss 0.5733677232638001\n",
      "step 9020 loss 0.5510205712169409\n",
      "step 9040 loss 0.6150920744985342\n",
      "step 9060 loss 0.6485845148563385\n",
      "step 9080 loss 0.5479447104036808\n",
      "step 9100 loss 0.5492219634354114\n",
      "step 9120 loss 0.5371297962963582\n",
      "step 9140 loss 0.5468714851886034\n",
      "step 9160 loss 0.5617689896374941\n",
      "step 9180 loss 0.5847058042883873\n",
      "step 9200 loss 0.6411392994225025\n",
      "step 9220 loss 0.5171500131487846\n",
      "step 9240 loss 0.4957826090976596\n",
      "step 9260 loss 0.588179699331522\n",
      "step 9280 loss 0.5359471671283245\n",
      "step 9300 loss 0.43947269693017005\n",
      "step 9320 loss 0.5871563624590636\n",
      "step 9340 loss 0.5584793476387858\n",
      "step 9360 loss 0.5123178593814373\n",
      "step 9380 loss 0.4531996624544263\n",
      "step 9400 loss 0.5556345656514168\n",
      "step 9420 loss 0.5734372332692146\n",
      "step 9440 loss 0.5936369217932225\n",
      "step 9460 loss 0.5163227081298828\n",
      "step 9480 loss 0.4624020643532276\n",
      "step 9500 loss 0.39063012450933454\n",
      "step 9520 loss 0.49718616232275964\n",
      "step 9540 loss 0.3833813307806849\n",
      "step 9560 loss 0.5530057217925787\n",
      "step 9580 loss 0.5166673794388771\n",
      "step 9600 loss 0.47177127487957476\n",
      "step 9620 loss 0.4991732008755207\n",
      "step 9640 loss 0.45797025514766576\n",
      "step 9660 loss 0.5233008384704589\n",
      "step 9680 loss 0.42422295212745664\n",
      "step 9700 loss 0.532860665768385\n",
      "step 9720 loss 0.5369158197194338\n",
      "step 9740 loss 0.49769065603613855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9760 loss 0.5437453594058752\n",
      "step 9780 loss 0.4725322399288416\n",
      "step 9800 loss 0.39577089473605154\n",
      "step 9820 loss 0.5434926800429821\n",
      "step 9840 loss 0.6477682664990425\n",
      "step 9860 loss 0.45388770997524264\n",
      "step 9880 loss 0.6213390033692121\n",
      "step 9900 loss 0.5369862887077034\n",
      "step 9920 loss 0.5711780320852995\n",
      "step 9940 loss 0.5149630881845951\n",
      "step 9960 loss 0.46584524437785146\n",
      "step 9980 loss 0.5594268836081028\n",
      "step 10000 loss 0.5472898080945015\n",
      "step 10020 loss 0.3854832325130701\n",
      "step 10040 loss 0.5152585089206696\n",
      "step 10060 loss 0.38435007641091945\n",
      "step 10080 loss 0.38758729174733164\n",
      "step 10100 loss 0.37701262906193733\n",
      "step 10120 loss 0.5778123959898949\n",
      "step 10140 loss 0.49014346916228535\n",
      "step 10160 loss 0.5743768196552992\n",
      "step 10180 loss 0.448729232698679\n",
      "step 10200 loss 0.6531313240528107\n",
      "step 10220 loss 0.5236358620226383\n",
      "step 10240 loss 0.4262387868016958\n",
      "step 10260 loss 0.48917985484004023\n",
      "step 10280 loss 0.44405491426587107\n",
      "step 10300 loss 0.41068476922810077\n",
      "step 10320 loss 0.4127339392900467\n",
      "step 10340 loss 0.5474960848689079\n",
      "step 10360 loss 0.47824172116816044\n",
      "step 10380 loss 0.5445577472448349\n",
      "step 10400 loss 0.36741891466081145\n",
      "step 10420 loss 0.4639456797391176\n",
      "step 10440 loss 0.5809599235653877\n",
      "step 10460 loss 0.49557920917868614\n",
      "step 10480 loss 0.5246572591364383\n",
      "step 10500 loss 0.5682398710399866\n",
      "step 10520 loss 0.5029890097677707\n",
      "step 10540 loss 0.5109733723104\n",
      "step 10560 loss 0.4188195403665304\n",
      "step 10580 loss 0.5278853829950094\n",
      "step 10600 loss 0.45802330262959\n",
      "step 10620 loss 0.48785033635795116\n",
      "step 10640 loss 0.5427144601941108\n",
      "step 10660 loss 0.45752062536776067\n",
      "step 10680 loss 0.3960088711231947\n",
      "step 10700 loss 0.5371076021343469\n",
      "step 10720 loss 0.5075457528233528\n",
      "step 10740 loss 0.40454565165564416\n",
      "step 10760 loss 0.4903744734823704\n",
      "step 10780 loss 0.5522728368639946\n",
      "step 10800 loss 0.4912778429687023\n",
      "step 10820 loss 0.37045207601040603\n",
      "step 10840 loss 0.4575366210192442\n",
      "step 10860 loss 0.39455314856022594\n",
      "step 10880 loss 0.45759309027343986\n",
      "step 10900 loss 0.538247088715434\n",
      "step 10920 loss 0.39425548668950794\n",
      "step 10940 loss 0.4901804244145751\n",
      "step 10960 loss 0.46384940296411514\n",
      "step 10980 loss 0.5599326889961957\n",
      "step 11000 loss 0.48830169439315796\n",
      "step 11020 loss 0.36282992772758005\n",
      "step 11040 loss 0.5056537640281021\n",
      "step 11060 loss 0.4873677007853985\n",
      "step 11080 loss 0.36430611815303565\n",
      "step 11100 loss 0.4427525855600834\n",
      "step 11120 loss 0.48492878340184686\n",
      "step 11140 loss 0.41283198446035385\n",
      "step 11160 loss 0.5689771777018905\n",
      "step 11180 loss 0.4394597072154284\n",
      "step 11200 loss 0.3813413862138987\n",
      "step 11220 loss 0.43427459318190814\n",
      "step 11240 loss 0.33777797520160674\n",
      "step 11260 loss 0.46959129739552735\n",
      "step 11280 loss 0.473187430948019\n",
      "step 11300 loss 0.48189877048134805\n",
      "step 11320 loss 0.38826352916657925\n",
      "step 11340 loss 0.49671756438910963\n",
      "step 11360 loss 0.47640441209077833\n",
      "step 11380 loss 0.4156371206045151\n",
      "step 11400 loss 0.5969145443290472\n",
      "step 11420 loss 0.42934710085391997\n",
      "step 11440 loss 0.4995665822178125\n",
      "step 11460 loss 0.4272779330611229\n",
      "step 11480 loss 0.40744584426283836\n",
      "step 11500 loss 0.3426311491057277\n",
      "step 11520 loss 0.35421272832900286\n",
      "step 11540 loss 0.31670445296913385\n",
      "step 11560 loss 0.4488931134343147\n",
      "step 11580 loss 0.38578131385147574\n",
      "step 11600 loss 0.3961301129311323\n",
      "step 11620 loss 0.45357653871178627\n",
      "step 11640 loss 0.34276283187791706\n",
      "step 11660 loss 0.4326432399451733\n",
      "step 11680 loss 0.42039501322433354\n",
      "step 11700 loss 0.431932383030653\n",
      "step 11720 loss 0.38557056747376917\n",
      "step 11740 loss 0.48895525811240076\n",
      "step 11760 loss 0.3713199028745294\n",
      "step 11780 loss 0.609815276414156\n",
      "step 11800 loss 0.4295122645795345\n",
      "step 11820 loss 0.3649941585958004\n",
      "step 11840 loss 0.4856800176203251\n",
      "step 11860 loss 0.41946901082992555\n",
      "step 11880 loss 0.2885479789227247\n",
      "step 11900 loss 0.34127296176739036\n",
      "step 11920 loss 0.45445597264915705\n",
      "step 11940 loss 0.5100012332201004\n",
      "step 11960 loss 0.39248966071754693\n",
      "step 11980 loss 0.40504103936254976\n",
      "step 12000 loss 0.4563011910766363\n",
      "step 12020 loss 0.4367818864062428\n",
      "step 12040 loss 0.3950747691094875\n",
      "step 12060 loss 0.3690404869616032\n",
      "step 12080 loss 0.4164989821612835\n",
      "step 12100 loss 0.3631156740710139\n",
      "step 12120 loss 0.2492877272889018\n",
      "step 12140 loss 0.4478287845849991\n",
      "step 12160 loss 0.5091559805907309\n",
      "step 12180 loss 0.5136007696390152\n",
      "step 12200 loss 0.38085461109876634\n",
      "step 12220 loss 0.47635281533002855\n",
      "step 12240 loss 0.43010354563593867\n",
      "step 12260 loss 0.48754746466875076\n",
      "step 12280 loss 0.44560116678476336\n",
      "step 12300 loss 0.43197699934244155\n",
      "step 12320 loss 0.5090958997607231\n",
      "step 12340 loss 0.33084265887737274\n",
      "step 12360 loss 0.42085774466395376\n",
      "step 12380 loss 0.38135131625458596\n",
      "step 12400 loss 0.4063433388248086\n",
      "step 12420 loss 0.46347714532166717\n",
      "step 12440 loss 0.3441495142877102\n",
      "step 12460 loss 0.44591783480718733\n",
      "step 12480 loss 0.37394776176661254\n",
      "step 12500 loss 0.2988451648503542\n",
      "step 12520 loss 0.5012844977900386\n",
      "step 12540 loss 0.3855800312012434\n",
      "step 12560 loss 0.43049210887402295\n",
      "step 12580 loss 0.5628862708806992\n",
      "step 12600 loss 0.4395662549883127\n",
      "step 12620 loss 0.444741552323103\n",
      "step 12640 loss 0.35356028787791727\n",
      "step 12660 loss 0.4612344976514578\n",
      "step 12680 loss 0.3551919309422374\n",
      "step 12700 loss 0.47502370066940786\n",
      "step 12720 loss 0.44485321938991546\n",
      "step 12740 loss 0.3308829039335251\n",
      "step 12760 loss 0.4068891920149326\n",
      "step 12780 loss 0.30547068510204556\n",
      "step 12800 loss 0.3561924729496241\n",
      "step 12820 loss 0.4043496560305357\n",
      "step 12840 loss 0.4030049335211515\n",
      "step 12860 loss 0.35230788104236127\n",
      "step 12880 loss 0.3623704623430967\n",
      "step 12900 loss 0.3643374122679234\n",
      "step 12920 loss 0.3769143189303577\n",
      "step 12940 loss 0.427620829641819\n",
      "step 12960 loss 0.3666049223393202\n",
      "step 12980 loss 0.3519032340496778\n",
      "step 13000 loss 0.27183790896087884\n",
      "step 13020 loss 0.3486352901905775\n",
      "step 13040 loss 0.4551143690943718\n",
      "step 13060 loss 0.37985532488673923\n",
      "step 13080 loss 0.31300499886274336\n",
      "step 13100 loss 0.3830876981839538\n",
      "step 13120 loss 0.39089690782129766\n",
      "step 13140 loss 0.4206502373795956\n",
      "step 13160 loss 0.32616679575294255\n",
      "step 13180 loss 0.4421127401292324\n",
      "step 13200 loss 0.4355853971093893\n",
      "step 13220 loss 0.4448641583323479\n",
      "step 13240 loss 0.41910931169986726\n",
      "step 13260 loss 0.4120031738653779\n",
      "step 13280 loss 0.4295149687677622\n",
      "step 13300 loss 0.4332750944420695\n",
      "step 13320 loss 0.43516356982290744\n",
      "step 13340 loss 0.42689174562692644\n",
      "step 13360 loss 0.2744876847602427\n",
      "step 13380 loss 0.48690848853439095\n",
      "step 13400 loss 0.4026828346773982\n",
      "step 13420 loss 0.44322694893926384\n",
      "step 13440 loss 0.47078507505357264\n",
      "step 13460 loss 0.44939206428825856\n",
      "step 13480 loss 0.38495545387268065\n",
      "step 13500 loss 0.4263741446658969\n",
      "step 13520 loss 0.5073586445301771\n",
      "step 13540 loss 0.35035845525562764\n",
      "step 13560 loss 0.4092673618346453\n",
      "step 13580 loss 0.4076829694211483\n",
      "step 13600 loss 0.39526518881320954\n",
      "step 13620 loss 0.47499691769480706\n",
      "step 13640 loss 0.5430480733513832\n",
      "step 13660 loss 0.4057778317481279\n",
      "step 13680 loss 0.36960934810340407\n",
      "step 13700 loss 0.47947570160031316\n",
      "step 13720 loss 0.34870925517752765\n",
      "step 13740 loss 0.3738004371523857\n",
      "step 13760 loss 0.38618626445531845\n",
      "step 13780 loss 0.545194448903203\n",
      "step 13800 loss 0.4245103938505054\n",
      "step 13820 loss 0.430156066454947\n",
      "step 13840 loss 0.38091780357062816\n",
      "step 13860 loss 0.37698233844712375\n",
      "step 13880 loss 0.32575934045016763\n",
      "step 13900 loss 0.3647947510704398\n",
      "step 13920 loss 0.31727359630167484\n",
      "step 13940 loss 0.3769575878046453\n",
      "step 13960 loss 0.3678672820329666\n",
      "step 13980 loss 0.31840340346097945\n",
      "step 14000 loss 0.31627573519945146\n",
      "step 14020 loss 0.38459718264639375\n",
      "step 14040 loss 0.30634401738643646\n",
      "step 14060 loss 0.41636315975338223\n",
      "step 14080 loss 0.4565355099737644\n",
      "step 14100 loss 0.30527278520166873\n",
      "step 14120 loss 0.3493989395909011\n",
      "step 14140 loss 0.2649052578024566\n",
      "step 14160 loss 0.3232544414699078\n",
      "step 14180 loss 0.3348966550081968\n",
      "step 14200 loss 0.3459240458905697\n",
      "step 14220 loss 0.3010588172823191\n",
      "step 14240 loss 0.4172676905989647\n",
      "step 14260 loss 0.3158896755427122\n",
      "step 14280 loss 0.3406960011459887\n",
      "step 14300 loss 0.385341804753989\n",
      "step 14320 loss 0.37010130789130924\n",
      "step 14340 loss 0.3100981106981635\n",
      "step 14360 loss 0.30854561664164065\n",
      "step 14380 loss 0.27006664518266915\n",
      "step 14400 loss 0.3150958243757486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14420 loss 0.42521117962896826\n",
      "step 14440 loss 0.4861670546233654\n",
      "step 14460 loss 0.3129865301772952\n",
      "step 14480 loss 0.4660312200896442\n",
      "step 14500 loss 0.39245738722383977\n",
      "step 14520 loss 0.3735300075262785\n",
      "step 14540 loss 0.32824043473228814\n",
      "step 14560 loss 0.32301296377554534\n",
      "step 14580 loss 0.27353907357901336\n",
      "step 14600 loss 0.38654768019914626\n",
      "step 14620 loss 0.3145447187125683\n",
      "step 14640 loss 0.3931882468983531\n",
      "step 14660 loss 0.34590654689818623\n",
      "step 14680 loss 0.40333166169002654\n",
      "step 14700 loss 0.2734866398386657\n",
      "step 14720 loss 0.39375204518437384\n",
      "step 14740 loss 0.40275752209126947\n",
      "step 14760 loss 0.3633188549429178\n",
      "step 14780 loss 0.3613278169184923\n",
      "step 14800 loss 0.38932064762338997\n",
      "step 14820 loss 0.2367310424335301\n",
      "step 14840 loss 0.336702230013907\n",
      "step 14860 loss 0.3455478329211473\n",
      "step 14880 loss 0.27943711122497916\n",
      "step 14900 loss 0.4675520084798336\n",
      "step 14920 loss 0.31843536123633387\n",
      "step 14940 loss 0.3601026427000761\n",
      "step 14960 loss 0.3232391245663166\n",
      "step 14980 loss 0.24039573934860528\n",
      "step 15000 loss 0.4211870079860091\n",
      "step 15020 loss 0.4983211954124272\n",
      "step 15040 loss 0.2898504415526986\n",
      "step 15060 loss 0.38783440347760917\n",
      "step 15080 loss 0.47064658962190153\n",
      "step 15100 loss 0.30341487042605875\n",
      "step 15120 loss 0.3972147863358259\n",
      "step 15140 loss 0.29680587984621526\n",
      "step 15160 loss 0.35913362819701433\n",
      "step 15180 loss 0.3293145555071533\n",
      "step 15200 loss 0.2985819948837161\n",
      "step 15220 loss 0.3541685724630952\n",
      "step 15240 loss 0.5215927615761757\n",
      "step 15260 loss 0.38498402629047634\n",
      "step 15280 loss 0.34600429572165015\n",
      "step 15300 loss 0.32908748015761374\n",
      "step 15320 loss 0.311287834495306\n",
      "step 15340 loss 0.4136567831970751\n",
      "step 15360 loss 0.40474006850272415\n",
      "step 15380 loss 0.43594032879918815\n",
      "step 15400 loss 0.396577550470829\n",
      "step 15420 loss 0.27723378688097\n",
      "step 15440 loss 0.35518870390951635\n",
      "step 15460 loss 0.3643729815259576\n",
      "step 15480 loss 0.45132583202794196\n",
      "step 15500 loss 0.3170136848464608\n",
      "step 15520 loss 0.3159956267103553\n",
      "step 15540 loss 0.3981954077258706\n",
      "step 15560 loss 0.44471146576106546\n",
      "step 15580 loss 0.3473690263926983\n",
      "step 15600 loss 0.3102149210870266\n",
      "step 15620 loss 0.417899657972157\n",
      "step 15640 loss 0.28582648234441876\n",
      "step 15660 loss 0.36794420182704923\n",
      "step 15680 loss 0.3856549672782421\n",
      "step 15700 loss 0.30801738798618317\n",
      "step 15720 loss 0.3576635722070932\n",
      "step 15740 loss 0.4006097879260778\n",
      "step 15760 loss 0.283371764048934\n",
      "step 15780 loss 0.27979044485837223\n",
      "step 15800 loss 0.26061692237854006\n",
      "step 15820 loss 0.29004917573183775\n",
      "step 15840 loss 0.3666733462363482\n",
      "step 15860 loss 0.2945795772597194\n",
      "step 15880 loss 0.3196513708680868\n",
      "step 15900 loss 0.29885981045663357\n",
      "step 15920 loss 0.35381149090826514\n",
      "step 15940 loss 0.4673192463815212\n",
      "step 15960 loss 0.2789554763585329\n",
      "step 15980 loss 0.4392744105309248\n",
      "step 16000 loss 0.39163614134304225\n",
      "step 16020 loss 0.32623730301856996\n",
      "step 16040 loss 0.38190704621374605\n",
      "step 16060 loss 0.3604376656934619\n",
      "step 16080 loss 0.4360967084765434\n",
      "step 16100 loss 0.4773812934756279\n",
      "step 16120 loss 0.38340496458113194\n",
      "step 16140 loss 0.25161747727543116\n",
      "step 16160 loss 0.2988957980647683\n",
      "step 16180 loss 0.3222067976370454\n",
      "step 16200 loss 0.346509936824441\n",
      "step 16220 loss 0.3543962297961116\n",
      "step 16240 loss 0.29540627133101227\n",
      "step 16260 loss 0.3145374508574605\n",
      "step 16280 loss 0.343257081322372\n",
      "step 16300 loss 0.4710976203903556\n",
      "step 16320 loss 0.35519383251667025\n",
      "step 16340 loss 0.3344821631908417\n",
      "step 16360 loss 0.5019605059176684\n",
      "step 16380 loss 0.26846932154148817\n",
      "step 16400 loss 0.3703361278399825\n",
      "step 16420 loss 0.33978280341252687\n",
      "step 16440 loss 0.37176283448934555\n",
      "step 16460 loss 0.350181395187974\n",
      "step 16480 loss 0.3252085910178721\n",
      "step 16500 loss 0.33024175819009544\n",
      "step 16520 loss 0.3081580840051174\n",
      "step 16540 loss 0.35317818373441695\n",
      "step 16560 loss 0.3259117273613811\n",
      "step 16580 loss 0.30436360612511637\n",
      "step 16600 loss 0.277457488514483\n",
      "step 16620 loss 0.289357234351337\n",
      "step 16640 loss 0.3054669271223247\n",
      "step 16660 loss 0.370269404258579\n",
      "step 16680 loss 0.2982399613596499\n",
      "step 16700 loss 0.31950009828433396\n",
      "step 16720 loss 0.27775112553499637\n",
      "step 16740 loss 0.40205492051318287\n",
      "step 16760 loss 0.4188678379636258\n",
      "step 16780 loss 0.419955100864172\n",
      "step 16800 loss 0.3223084554076195\n",
      "step 16820 loss 0.3107654666528106\n",
      "step 16840 loss 0.30053864400833846\n",
      "step 16860 loss 0.4164807917550206\n",
      "step 16880 loss 0.2872660884167999\n",
      "step 16900 loss 0.2992413470987231\n",
      "step 16920 loss 0.3061903115361929\n",
      "step 16940 loss 0.3626049365848303\n",
      "step 16960 loss 0.3328074110671878\n",
      "step 16980 loss 0.4007403607480228\n",
      "step 17000 loss 0.32675925046205523\n",
      "step 17020 loss 0.24963242127560079\n",
      "step 17040 loss 0.33470149505883456\n",
      "step 17060 loss 0.3183403819799423\n",
      "step 17080 loss 0.29395487532019615\n",
      "step 17100 loss 0.3803021885454655\n",
      "step 17120 loss 0.37189076095819473\n",
      "step 17140 loss 0.34691082444041965\n",
      "step 17160 loss 0.2740441243164241\n",
      "step 17180 loss 0.3296509929001331\n",
      "step 17200 loss 0.3136479835025966\n",
      "step 17220 loss 0.3243424104060978\n",
      "step 17240 loss 0.3964624861255288\n",
      "step 17260 loss 0.325825497508049\n",
      "step 17280 loss 0.31290829423815014\n",
      "step 17300 loss 0.36204234808683394\n",
      "step 17320 loss 0.27444983273744583\n",
      "step 17340 loss 0.24380365256220102\n",
      "step 17360 loss 0.34061796572059394\n",
      "step 17380 loss 0.3996102553792298\n",
      "step 17400 loss 0.255479917419143\n",
      "step 17420 loss 0.2721661664545536\n",
      "step 17440 loss 0.3220718548633158\n",
      "step 17460 loss 0.26260738805867734\n",
      "step 17480 loss 0.3829686141572893\n",
      "step 17500 loss 0.2630179587751627\n",
      "step 17520 loss 0.37039232281968\n",
      "step 17540 loss 0.4551781311631203\n",
      "step 17560 loss 0.3342942150309682\n",
      "step 17580 loss 0.34716458404436706\n",
      "step 17600 loss 0.38150520576164126\n",
      "step 17620 loss 0.3913022965192795\n",
      "step 17640 loss 0.2553762551397085\n",
      "step 17660 loss 0.32836482897400854\n",
      "step 17680 loss 0.27067953404039147\n",
      "step 17700 loss 0.25999756827950476\n",
      "step 17720 loss 0.31633054930716753\n",
      "step 17740 loss 0.4139285162091255\n",
      "step 17760 loss 0.34077088013291357\n",
      "step 17780 loss 0.3090692483820021\n",
      "step 17800 loss 0.28039292320609094\n",
      "step 17820 loss 0.4078842895105481\n",
      "step 17840 loss 0.25072271600365637\n",
      "step 17860 loss 0.2835907837375998\n",
      "step 17880 loss 0.3282022192142904\n",
      "step 17900 loss 0.45262903459370135\n",
      "step 17920 loss 0.3613526977598667\n",
      "step 17940 loss 0.2432211604900658\n",
      "step 17960 loss 0.3153738796710968\n",
      "step 17980 loss 0.28228785321116445\n",
      "step 18000 loss 0.3739729566499591\n",
      "step 18020 loss 0.2432101651560515\n",
      "step 18040 loss 0.3309289735741913\n",
      "step 18060 loss 0.3161940099671483\n",
      "step 18080 loss 0.3139984991401434\n",
      "step 18100 loss 0.3068340389057994\n",
      "step 18120 loss 0.3080458348616958\n",
      "step 18140 loss 0.3802557561546564\n",
      "step 18160 loss 0.2532105667516589\n",
      "step 18180 loss 0.4410146750509739\n",
      "step 18200 loss 0.30718156043440104\n",
      "step 18220 loss 0.2971726354211569\n",
      "step 18240 loss 0.40046204924583434\n",
      "step 18260 loss 0.3217329868115485\n",
      "step 18280 loss 0.2442445178516209\n",
      "step 18300 loss 0.28230144269764423\n",
      "step 18320 loss 0.24930349607020616\n",
      "step 18340 loss 0.23405844597145914\n",
      "step 18360 loss 0.3046750154346228\n",
      "step 18380 loss 0.2690970815718174\n",
      "step 18400 loss 0.3322861632332206\n",
      "step 18420 loss 0.3461299051530659\n",
      "step 18440 loss 0.3664469687733799\n",
      "step 18460 loss 0.26984647698700426\n",
      "step 18480 loss 0.36080027315765617\n",
      "step 18500 loss 0.2293148148804903\n",
      "step 18520 loss 0.25512983789667487\n",
      "step 18540 loss 0.3341363713145256\n",
      "step 18560 loss 0.30698129180818795\n",
      "step 18580 loss 0.3146555170416832\n",
      "step 18600 loss 0.39075631387531756\n",
      "step 18620 loss 0.3576561652123928\n",
      "step 18640 loss 0.2677023965865374\n",
      "step 18660 loss 0.3073133774101734\n",
      "step 18680 loss 0.3305989961139858\n",
      "step 18700 loss 0.35188691769726577\n",
      "step 18720 loss 0.34539425317198036\n",
      "step 18740 loss 0.1807300403015688\n",
      "step 18760 loss 0.30633631150703877\n",
      "step 18780 loss 0.29086857438087466\n",
      "step 18800 loss 0.41924824453890325\n",
      "step 18820 loss 0.33917469773441555\n",
      "step 18840 loss 0.36438539866358044\n",
      "step 18860 loss 0.26247182451188567\n",
      "step 18880 loss 0.37015631096437573\n",
      "step 18900 loss 0.3990768577903509\n",
      "step 18920 loss 0.33632954889908434\n",
      "step 18940 loss 0.27767100650817156\n",
      "step 18960 loss 0.33247016184031963\n",
      "step 18980 loss 0.21814391417428852\n",
      "step 19000 loss 0.41175600830465553\n",
      "step 19020 loss 0.31715072561055424\n",
      "step 19040 loss 0.3890374682843685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19060 loss 0.2880386605858803\n",
      "step 19080 loss 0.27087020529434086\n",
      "step 19100 loss 0.25550264017656443\n",
      "step 19120 loss 0.30119307078421115\n",
      "step 19140 loss 0.31998858265578745\n",
      "step 19160 loss 0.3213635457213968\n",
      "step 19180 loss 0.34025832479819657\n",
      "step 19200 loss 0.20846925107762218\n",
      "step 19220 loss 0.18007696145214142\n",
      "step 19240 loss 0.35490981237962843\n",
      "step 19260 loss 0.2969955254346132\n",
      "step 19280 loss 0.23687245529145\n",
      "step 19300 loss 0.3015058010816574\n",
      "step 19320 loss 0.3274524969048798\n",
      "step 19340 loss 0.17902935678139328\n",
      "step 19360 loss 0.2997012242209166\n",
      "step 19380 loss 0.2946732981130481\n",
      "step 19400 loss 0.3295608405023813\n",
      "step 19420 loss 0.3338103686459363\n",
      "step 19440 loss 0.36814675251953305\n",
      "step 19460 loss 0.2806670473888516\n",
      "step 19480 loss 0.26299281883984804\n",
      "step 19500 loss 0.38605516669340434\n",
      "step 19520 loss 0.2604771500453353\n",
      "step 19540 loss 0.2365845251828432\n",
      "step 19560 loss 0.32690901644527914\n",
      "step 19580 loss 0.3539141910150647\n",
      "step 19600 loss 0.2826032777316868\n",
      "step 19620 loss 0.3284463727613911\n",
      "step 19640 loss 0.3714063689112663\n",
      "step 19660 loss 0.372251440025866\n",
      "step 19680 loss 0.2771279246546328\n",
      "step 19700 loss 0.27308861748315394\n",
      "step 19720 loss 0.2983523250557482\n",
      "step 19740 loss 0.2855040395632386\n",
      "step 19760 loss 0.3440395751968026\n",
      "step 19780 loss 0.289840083476156\n",
      "step 19800 loss 0.322395286988467\n",
      "step 19820 loss 0.3301113192923367\n",
      "step 19840 loss 0.27182369120419025\n",
      "step 19860 loss 0.2727279213257134\n",
      "step 19880 loss 0.34869925379753114\n",
      "step 19900 loss 0.3009512582793832\n",
      "step 19920 loss 0.3119602970778942\n",
      "step 19940 loss 0.3457310160622001\n",
      "step 19960 loss 0.2634119417052716\n",
      "step 19980 loss 0.29403630141168835\n",
      "step 20000 loss 0.3462012563832104\n",
      "step 20020 loss 0.2809634905308485\n",
      "step 20040 loss 0.38083115182816984\n",
      "step 20060 loss 0.32731028981506827\n",
      "step 20080 loss 0.28580108741298316\n",
      "step 20100 loss 0.29959864243865014\n",
      "step 20120 loss 0.28435521624051036\n",
      "step 20140 loss 0.2256866414565593\n",
      "step 20160 loss 0.3011249676346779\n",
      "step 20180 loss 0.2859015934169292\n",
      "step 20200 loss 0.31452173441648484\n",
      "step 20220 loss 0.32579049505293367\n",
      "step 20240 loss 0.34276435188949106\n",
      "step 20260 loss 0.32702634362503885\n",
      "step 20280 loss 0.2590362375602126\n",
      "step 20300 loss 0.2694510345347226\n",
      "step 20320 loss 0.22658800245262684\n",
      "step 20340 loss 0.2457978312857449\n",
      "step 20360 loss 0.22742174230515957\n",
      "step 20380 loss 0.3412893365137279\n",
      "step 20400 loss 0.32516539106145503\n",
      "step 20420 loss 0.342410352639854\n",
      "step 20440 loss 0.3316098002716899\n",
      "step 20460 loss 0.31060504033230246\n",
      "step 20480 loss 0.3316089939326048\n",
      "step 20500 loss 0.2816413545049727\n",
      "step 20520 loss 0.26717240940779446\n",
      "step 20540 loss 0.26909702755510806\n",
      "step 20560 loss 0.3709128491580486\n",
      "step 20580 loss 0.2421906410716474\n",
      "step 20600 loss 0.18851335803046823\n",
      "step 20620 loss 0.2909336945042014\n",
      "step 20640 loss 0.22606436228379606\n",
      "step 20660 loss 0.19561774106696247\n",
      "step 20680 loss 0.2944791901856661\n",
      "step 20700 loss 0.27187567595392464\n",
      "step 20720 loss 0.25998883731663225\n",
      "step 20740 loss 0.37439726777374743\n",
      "step 20760 loss 0.23114633727818729\n",
      "step 20780 loss 0.28000363055616617\n",
      "step 20800 loss 0.26519393948838116\n",
      "step 20820 loss 0.31150968400761486\n",
      "step 20840 loss 0.26206851825118066\n",
      "step 20860 loss 0.3655883386731148\n",
      "step 20880 loss 0.27276372860651465\n",
      "step 20900 loss 0.16442036819644273\n",
      "step 20920 loss 0.19688128707930447\n",
      "step 20940 loss 0.27396528217941524\n",
      "step 20960 loss 0.25804985836148264\n",
      "step 20980 loss 0.22890551779419183\n",
      "step 21000 loss 0.3028223068453372\n",
      "step 21020 loss 0.29460616102442144\n",
      "step 21040 loss 0.26137407436035576\n",
      "step 21060 loss 0.31789862308651207\n",
      "step 21080 loss 0.2547490055672824\n",
      "step 21100 loss 0.33254959443584087\n",
      "step 21120 loss 0.25778265073895457\n",
      "step 21140 loss 0.37206288808956745\n",
      "step 21160 loss 0.27636222913861275\n",
      "step 21180 loss 0.24435721915215253\n",
      "step 21200 loss 0.2512489202432334\n",
      "step 21220 loss 0.27650097412988545\n",
      "step 21240 loss 0.30441592950373886\n",
      "step 21260 loss 0.3110016311518848\n",
      "step 21280 loss 0.3003508830210194\n",
      "step 21300 loss 0.2892179163172841\n",
      "step 21320 loss 0.32798778750002383\n",
      "step 21340 loss 0.3957184445578605\n",
      "step 21360 loss 0.23769201906397938\n",
      "step 21380 loss 0.14670380093157293\n",
      "step 21400 loss 0.23970403810963034\n",
      "step 21420 loss 0.3180306245572865\n",
      "step 21440 loss 0.25718513773754237\n",
      "step 21460 loss 0.26204928373917935\n",
      "step 21480 loss 0.2740274528972805\n",
      "step 21500 loss 0.31778196673840287\n",
      "step 21520 loss 0.30419092616066334\n",
      "step 21540 loss 0.3325685548596084\n",
      "step 21560 loss 0.305702749453485\n",
      "step 21580 loss 0.2326523275114596\n",
      "step 21600 loss 0.18248189294245093\n",
      "step 21620 loss 0.2860087651759386\n",
      "step 21640 loss 0.3516018385067582\n",
      "step 21660 loss 0.2470678348094225\n",
      "step 21680 loss 0.26279386356472967\n",
      "step 21700 loss 0.3041329028084874\n",
      "step 21720 loss 0.33096140353009107\n",
      "step 21740 loss 0.23207743894308805\n",
      "step 21760 loss 0.3730073107406497\n",
      "step 21780 loss 0.3037169985473156\n",
      "step 21800 loss 0.19640801697969437\n",
      "step 21820 loss 0.1823325404897332\n",
      "step 21840 loss 0.24784532678313553\n",
      "step 21860 loss 0.17940349966520444\n",
      "step 21880 loss 0.24238985287956893\n",
      "step 21900 loss 0.28699583411216734\n",
      "step 21920 loss 0.2525277826935053\n",
      "step 21940 loss 0.19066753294318914\n",
      "step 21960 loss 0.3814872395247221\n",
      "step 21980 loss 0.2579754779115319\n",
      "step 22000 loss 0.3114630341529846\n",
      "step 22020 loss 0.22201301520690322\n",
      "step 22040 loss 0.2125493231229484\n",
      "step 22060 loss 0.31881178272888067\n",
      "step 22080 loss 0.22639904045499862\n",
      "step 22100 loss 0.3817780241370201\n",
      "step 22120 loss 0.34542719181627035\n",
      "step 22140 loss 0.21827025711536407\n",
      "step 22160 loss 0.23201907156035304\n",
      "step 22180 loss 0.23974271453917026\n",
      "step 22200 loss 0.2923779341392219\n",
      "step 22220 loss 0.21735778907313943\n",
      "step 22240 loss 0.3521365601569414\n",
      "step 22260 loss 0.3154449806548655\n",
      "step 22280 loss 0.29219891941174864\n",
      "step 22300 loss 0.3482618648558855\n",
      "step 22320 loss 0.28866003663279116\n",
      "step 22340 loss 0.2855425373651087\n",
      "step 22360 loss 0.3264046529307961\n",
      "step 22380 loss 0.344635046645999\n",
      "step 22400 loss 0.2905463794246316\n",
      "step 22420 loss 0.29543436896055936\n",
      "step 22440 loss 0.33158466778695583\n",
      "step 22460 loss 0.2341118549928069\n",
      "step 22480 loss 0.2927925240248442\n",
      "step 22500 loss 0.2572942198254168\n",
      "step 22520 loss 0.2195518319029361\n",
      "step 22540 loss 0.30680525824427607\n",
      "step 22560 loss 0.3348170225508511\n",
      "step 22580 loss 0.3212864841334522\n",
      "step 22600 loss 0.2746778817847371\n",
      "step 22620 loss 0.36302329823374746\n",
      "step 22640 loss 0.35614122496917844\n",
      "step 22660 loss 0.24934420585632325\n",
      "step 22680 loss 0.2602227326016873\n",
      "step 22700 loss 0.25934024224989116\n",
      "step 22720 loss 0.19301168848760425\n",
      "step 22740 loss 0.3403620579279959\n",
      "step 22760 loss 0.40118059478700163\n",
      "step 22780 loss 0.261242320202291\n",
      "step 22800 loss 0.2552661871537566\n",
      "step 22820 loss 0.29623283687978985\n",
      "step 22840 loss 0.24750320482999086\n",
      "step 22860 loss 0.25002728435210886\n",
      "step 22880 loss 0.2171523144003004\n",
      "step 22900 loss 0.2895202594809234\n",
      "step 22920 loss 0.3085328051354736\n",
      "step 22940 loss 0.45897251218557356\n",
      "step 22960 loss 0.3281662672758102\n",
      "step 22980 loss 0.2327371569350362\n",
      "step 23000 loss 0.15888171936385334\n",
      "step 23020 loss 0.24687393163330854\n",
      "step 23040 loss 0.2982503923587501\n",
      "step 23060 loss 0.2927452003583312\n",
      "step 23080 loss 0.23269096622243524\n",
      "step 23100 loss 0.2789760805666447\n",
      "step 23120 loss 0.27723560775630174\n",
      "step 23140 loss 0.2367247159127146\n",
      "step 23160 loss 0.28687796704471114\n",
      "step 23180 loss 0.27234326968900857\n",
      "step 23200 loss 0.22140194233506919\n",
      "step 23220 loss 0.21729197157546878\n",
      "step 23240 loss 0.26780466600321234\n",
      "step 23260 loss 0.25441773971542714\n",
      "step 23280 loss 0.23029674421995877\n",
      "step 23300 loss 0.2261596718803048\n",
      "step 23320 loss 0.2585834827274084\n",
      "step 23340 loss 0.16369321662932634\n",
      "step 23360 loss 0.23600178360939025\n",
      "step 23380 loss 0.2819963229820132\n",
      "step 23400 loss 0.2811290425248444\n",
      "step 23420 loss 0.23421539822593332\n",
      "step 23440 loss 0.24232543068937956\n",
      "step 23460 loss 0.28732187980785967\n",
      "step 23480 loss 0.2926866702735424\n",
      "step 23500 loss 0.32184113189578056\n",
      "step 23520 loss 0.25181061001494526\n",
      "step 23540 loss 0.24944966586772352\n",
      "step 23560 loss 0.2547511372715235\n",
      "step 23580 loss 0.24614428086206316\n",
      "step 23600 loss 0.3004163193749264\n",
      "step 23620 loss 0.24870855920016766\n",
      "step 23640 loss 0.26005597719922663\n",
      "step 23660 loss 0.23788882214576007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 23680 loss 0.22355380058288574\n",
      "step 23700 loss 0.17847079113125802\n",
      "step 23720 loss 0.24532783795148133\n",
      "step 23740 loss 0.3166666870936751\n",
      "step 23760 loss 0.20756356390193104\n",
      "step 23780 loss 0.27835907647386193\n",
      "step 23800 loss 0.19833559021353722\n",
      "step 23820 loss 0.281295269401744\n",
      "step 23840 loss 0.22231017190497368\n",
      "step 23860 loss 0.20305025877896696\n",
      "step 23880 loss 0.26177110970020295\n",
      "step 23900 loss 0.25656790342181923\n",
      "step 23920 loss 0.26556523055769504\n",
      "step 23940 loss 0.23538550173398107\n",
      "step 23960 loss 0.2028667011298239\n",
      "step 23980 loss 0.23361551351845264\n",
      "step 24000 loss 0.26086583230644467\n",
      "step 24020 loss 0.19791225623339415\n",
      "step 24040 loss 0.31000593106728047\n",
      "step 24060 loss 0.24100807383656503\n",
      "step 24080 loss 0.3003956195898354\n",
      "step 24100 loss 0.2277471374720335\n",
      "step 24120 loss 0.22398299314081668\n",
      "step 24140 loss 0.20315952943637966\n",
      "step 24160 loss 0.1695642032660544\n",
      "step 24180 loss 0.25393454935401677\n",
      "step 24200 loss 0.2029120283201337\n",
      "step 24220 loss 0.2543921545147896\n",
      "step 24240 loss 0.2690801367163658\n",
      "step 24260 loss 0.2617185108363628\n",
      "step 24280 loss 0.18471533269621432\n",
      "step 24300 loss 0.20859711039811374\n",
      "step 24320 loss 0.2621106546372175\n",
      "step 24340 loss 0.20048611089587212\n",
      "step 24360 loss 0.17987303603440524\n",
      "step 24380 loss 0.23473729589022696\n",
      "step 24400 loss 0.18481309111230076\n",
      "step 24420 loss 0.3197208309546113\n",
      "step 24440 loss 0.25337202097289263\n",
      "step 24460 loss 0.16245909640565515\n",
      "step 24480 loss 0.26395986657589676\n",
      "step 24500 loss 0.27644389048218726\n",
      "step 24520 loss 0.2248233207501471\n",
      "step 24540 loss 0.21489717047661544\n",
      "step 24560 loss 0.2141167014837265\n",
      "step 24580 loss 0.182249448960647\n",
      "step 24600 loss 0.24100449942052365\n",
      "step 24620 loss 0.15383744854480028\n",
      "step 24640 loss 0.2993306312710047\n",
      "step 24660 loss 0.24641930144280194\n",
      "step 24680 loss 0.2199092094786465\n",
      "step 24700 loss 0.18694598097354173\n",
      "step 24720 loss 0.27178232185542583\n",
      "step 24740 loss 0.29407276660203935\n",
      "step 24760 loss 0.1862772123888135\n",
      "step 24780 loss 0.2802540028002113\n",
      "step 24800 loss 0.23591847554780543\n",
      "step 24820 loss 0.19608951229602098\n",
      "step 24840 loss 0.22596154462080448\n",
      "step 24860 loss 0.29317051856778564\n",
      "step 24880 loss 0.24130094163119792\n",
      "step 24900 loss 0.199492604797706\n",
      "step 24920 loss 0.31801673662848773\n",
      "step 24940 loss 0.31830781437456607\n",
      "step 24960 loss 0.20995741225779058\n",
      "step 24980 loss 0.2821978561580181\n",
      "step 25000 loss 0.18094929244834929\n",
      "step 25020 loss 0.2533887857571244\n",
      "step 25040 loss 0.2188439516350627\n",
      "step 25060 loss 0.2034507215023041\n",
      "step 25080 loss 0.2900283203460276\n",
      "step 25100 loss 0.2950759781524539\n",
      "step 25120 loss 0.27985754259862006\n",
      "step 25140 loss 0.23082172460854053\n",
      "step 25160 loss 0.18714340790174902\n",
      "step 25180 loss 0.32135260296054186\n",
      "step 25200 loss 0.18470301288180052\n",
      "step 25220 loss 0.28608048902824523\n",
      "step 25240 loss 0.2791625190526247\n",
      "step 25260 loss 0.21546328533440828\n",
      "step 25280 loss 0.22941565588116647\n",
      "step 25300 loss 0.12266683848574758\n",
      "step 25320 loss 0.25045614130795\n",
      "step 25340 loss 0.2260271050501615\n",
      "step 25360 loss 0.32742824917659163\n",
      "step 25380 loss 0.24721476235426962\n",
      "step 25400 loss 0.3071758166188374\n",
      "step 25420 loss 0.2762081260792911\n",
      "step 25440 loss 0.19402099438011647\n",
      "step 25460 loss 0.20448456266894938\n",
      "step 25480 loss 0.25341499210335316\n",
      "step 25500 loss 0.24301422564312816\n",
      "step 25520 loss 0.23898225128650666\n",
      "step 25540 loss 0.22294124397449194\n",
      "step 25560 loss 0.23286934159696102\n",
      "step 25580 loss 0.2406027835793793\n",
      "step 25600 loss 0.2403618624433875\n",
      "step 25620 loss 0.2995863017626107\n",
      "step 25640 loss 0.25192508646287026\n",
      "step 25660 loss 0.2716167670208961\n",
      "step 25680 loss 0.19728386802598835\n",
      "step 25700 loss 0.25328824734315275\n",
      "step 25720 loss 0.29420233368873594\n",
      "step 25740 loss 0.31967789251357315\n",
      "step 25760 loss 0.20330772190354765\n",
      "step 25780 loss 0.27791041899472474\n",
      "step 25800 loss 0.1458102511242032\n",
      "step 25820 loss 0.23938293070532382\n",
      "step 25840 loss 0.3128005532547832\n",
      "step 25860 loss 0.2893694169819355\n",
      "step 25880 loss 0.22494121142663062\n",
      "step 25900 loss 0.19807244557887316\n",
      "step 25920 loss 0.23319571278989315\n",
      "step 25940 loss 0.28428766783326864\n",
      "step 25960 loss 0.20556373838335276\n",
      "step 25980 loss 0.21746325611602516\n",
      "step 26000 loss 0.20207062182016672\n",
      "step 26020 loss 0.34626869061030446\n",
      "step 26040 loss 0.2934396039694548\n",
      "step 26060 loss 0.2057974261464551\n",
      "step 26080 loss 0.2705915712751448\n",
      "step 26100 loss 0.25812835628166797\n",
      "step 26120 loss 0.22948667109012605\n",
      "step 26140 loss 0.2976977915968746\n",
      "step 26160 loss 0.26415259661152957\n",
      "step 26180 loss 0.23633079025894405\n",
      "step 26200 loss 0.21954096262343228\n",
      "step 26220 loss 0.22229404053650798\n",
      "step 26240 loss 0.14138068892061711\n",
      "step 26260 loss 0.17686594880651682\n",
      "step 26280 loss 0.27144915126264096\n",
      "step 26300 loss 0.1570358768105507\n",
      "step 26320 loss 0.24700981210917233\n",
      "step 26340 loss 0.3004141370765865\n",
      "step 26360 loss 0.20446374169550835\n",
      "step 26380 loss 0.2539631471969187\n",
      "step 26400 loss 0.1641975525766611\n",
      "step 26420 loss 0.264556048810482\n",
      "step 26440 loss 0.26829764265567063\n",
      "step 26460 loss 0.23435141500085593\n",
      "step 26480 loss 0.2842750122770667\n",
      "step 26500 loss 0.16268746303394438\n",
      "step 26520 loss 0.20345832775346934\n",
      "step 26540 loss 0.28832212593406437\n",
      "step 26560 loss 0.17609131830977276\n",
      "step 26580 loss 0.18760586362332105\n",
      "step 26600 loss 0.20464467722922564\n",
      "step 26620 loss 0.217075120145455\n",
      "step 26640 loss 0.18295815533492715\n",
      "step 26660 loss 0.25202785837464037\n",
      "step 26680 loss 0.35739346258342264\n",
      "step 26700 loss 0.27505080713890495\n",
      "step 26720 loss 0.23247961103916168\n",
      "step 26740 loss 0.2586219262215309\n",
      "step 26760 loss 0.2425373803358525\n",
      "step 26780 loss 0.2213174844160676\n",
      "step 26800 loss 0.15858358736149966\n",
      "step 26820 loss 0.2323610416613519\n",
      "step 26840 loss 0.29317123871296646\n",
      "step 26860 loss 0.23535208054818213\n",
      "step 26880 loss 0.2125264646485448\n",
      "step 26900 loss 0.20928374538198113\n",
      "step 26920 loss 0.23420836478471757\n",
      "step 26940 loss 0.2894130703993142\n",
      "step 26960 loss 0.301168551761657\n",
      "step 26980 loss 0.2180523920804262\n",
      "step 27000 loss 0.24659670908004044\n",
      "step 27020 loss 0.21791920689865946\n",
      "step 27040 loss 0.2871279831975698\n",
      "step 27060 loss 0.1919168929569423\n",
      "step 27080 loss 0.161917791236192\n",
      "step 27100 loss 0.22046317979693414\n",
      "step 27120 loss 0.2598962439224124\n",
      "step 27140 loss 0.1958712613210082\n",
      "step 27160 loss 0.3327070163562894\n",
      "step 27180 loss 0.1987869583070278\n",
      "step 27200 loss 0.27050439827144146\n",
      "step 27220 loss 0.1647305596852675\n",
      "step 27240 loss 0.29985229060985147\n",
      "step 27260 loss 0.21601001247763635\n",
      "step 27280 loss 0.25864079855382444\n",
      "step 27300 loss 0.26178024020046\n",
      "step 27320 loss 0.16601180778816343\n",
      "step 27340 loss 0.2011785997194238\n",
      "step 27360 loss 0.20432878511492164\n",
      "step 27380 loss 0.25709816049784423\n",
      "step 27400 loss 0.20212591860909015\n",
      "step 27420 loss 0.18248647982254623\n",
      "step 27440 loss 0.2535825056023896\n",
      "step 27460 loss 0.19838196709752082\n",
      "step 27480 loss 0.2592608002945781\n",
      "step 27500 loss 0.2023663366679102\n",
      "step 27520 loss 0.19522283594124018\n",
      "step 27540 loss 0.2026316245086491\n",
      "step 27560 loss 0.16510793683119118\n",
      "step 27580 loss 0.24164482895284892\n",
      "step 27600 loss 0.23205602839589118\n",
      "step 27620 loss 0.2506510964012705\n",
      "step 27640 loss 0.24866437031887473\n",
      "step 27660 loss 0.21779038596432657\n",
      "step 27680 loss 0.3045273205265403\n",
      "step 27700 loss 0.204986709728837\n",
      "step 27720 loss 0.23300565499812365\n",
      "step 27740 loss 0.23457099040970206\n",
      "step 27760 loss 0.18948044343851506\n",
      "step 27780 loss 0.21431546225212514\n",
      "step 27800 loss 0.2397075621993281\n",
      "step 27820 loss 0.13130490640178322\n",
      "step 27840 loss 0.20048620561137795\n",
      "step 27860 loss 0.19014532174915075\n",
      "step 27880 loss 0.35634593283757565\n",
      "step 27900 loss 0.19752103807404636\n",
      "step 27920 loss 0.23148804092779757\n",
      "step 27940 loss 0.22642688094638289\n",
      "step 27960 loss 0.37997670570621267\n",
      "step 27980 loss 0.1879282127134502\n",
      "step 28000 loss 0.21012419564649462\n",
      "step 28020 loss 0.28823866760358213\n",
      "step 28040 loss 0.2185652360320091\n",
      "step 28060 loss 0.265144745237194\n",
      "step 28080 loss 0.19508941881358624\n",
      "step 28100 loss 0.1835482918890193\n",
      "step 28120 loss 0.21610822202637792\n",
      "step 28140 loss 0.3086194251663983\n",
      "step 28160 loss 0.2269455290865153\n",
      "step 28180 loss 0.18081708485260606\n",
      "step 28200 loss 0.22089243903756142\n",
      "step 28220 loss 0.28994260327890514\n",
      "step 28240 loss 0.24471550746820867\n",
      "step 28260 loss 0.21287702079862356\n",
      "step 28280 loss 0.15590499751269818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28300 loss 0.28833422511816026\n",
      "step 28320 loss 0.30440572686493395\n",
      "step 28340 loss 0.18050468103028833\n",
      "step 28360 loss 0.2087681104429066\n",
      "step 28380 loss 0.324215782340616\n",
      "step 28400 loss 0.16543314922600985\n",
      "step 28420 loss 0.23209027564153076\n",
      "step 28440 loss 0.2226025358773768\n",
      "step 28460 loss 0.22479028441011906\n",
      "step 28480 loss 0.23920235394034534\n",
      "step 28500 loss 0.2290478221140802\n",
      "step 28520 loss 0.2159542660228908\n",
      "step 28540 loss 0.1910565703175962\n",
      "step 28560 loss 0.14084063302725552\n",
      "step 28580 loss 0.21380701363086702\n",
      "step 28600 loss 0.23083078414201735\n",
      "step 28620 loss 0.17404953073710203\n",
      "step 28640 loss 0.19052944835275412\n",
      "step 28660 loss 0.24981674228329212\n",
      "step 28680 loss 0.1881789357867092\n",
      "step 28700 loss 0.23619517269544305\n",
      "step 28720 loss 0.2091351472772658\n",
      "step 28740 loss 0.2016155523713678\n",
      "step 28760 loss 0.19398548319004477\n",
      "step 28780 loss 0.23050591219216585\n",
      "step 28800 loss 0.3134930341970176\n",
      "step 28820 loss 0.2856887564063072\n",
      "step 28840 loss 0.16912816604599357\n",
      "step 28860 loss 0.2228650783188641\n",
      "step 28880 loss 0.35932341958396136\n",
      "step 28900 loss 0.15494987620040773\n",
      "step 28920 loss 0.22882511620409787\n",
      "step 28940 loss 0.26245654579252004\n",
      "step 28960 loss 0.30083614657633007\n",
      "step 28980 loss 0.19300076114013792\n",
      "step 29000 loss 0.18934532813727856\n",
      "step 29020 loss 0.2312388762831688\n",
      "step 29040 loss 0.24228784441947937\n",
      "step 29060 loss 0.24599251435138286\n",
      "step 29080 loss 0.190130874956958\n",
      "step 29100 loss 0.1946158168837428\n",
      "step 29120 loss 0.2338951572775841\n",
      "step 29140 loss 0.24131166138686239\n",
      "step 29160 loss 0.2499515036237426\n",
      "step 29180 loss 0.20366194983944297\n",
      "step 29200 loss 0.2597274865955114\n",
      "step 29220 loss 0.2446728098206222\n",
      "step 29240 loss 0.13118934540543706\n",
      "step 29260 loss 0.1778191549703479\n",
      "step 29280 loss 0.23314900361001492\n",
      "step 29300 loss 0.23975786548107864\n",
      "step 29320 loss 0.21454445635899902\n",
      "step 29340 loss 0.2013128022197634\n",
      "step 29360 loss 0.1901465905830264\n",
      "step 29380 loss 0.21527029415592552\n",
      "step 29400 loss 0.23427831390872597\n",
      "step 29420 loss 0.2623662980273366\n",
      "step 29440 loss 0.24327077716588974\n",
      "step 29460 loss 0.1963798244483769\n",
      "step 29480 loss 0.19760052706114947\n",
      "step 29500 loss 0.17481029806658627\n",
      "step 29520 loss 0.20881108613684773\n",
      "step 29540 loss 0.21942172795534134\n",
      "step 29560 loss 0.2939887893386185\n",
      "step 29580 loss 0.20876726759597658\n",
      "step 29600 loss 0.1475724524818361\n",
      "step 29620 loss 0.2432358918711543\n",
      "step 29640 loss 0.26228064130991696\n",
      "step 29660 loss 0.1855690854601562\n",
      "step 29680 loss 0.20191765213385224\n",
      "step 29700 loss 0.23504666984081268\n",
      "step 29720 loss 0.16228840624680743\n",
      "step 29740 loss 0.24802588876336812\n",
      "step 29760 loss 0.2542867992073298\n",
      "step 29780 loss 0.20762846334837376\n",
      "step 29800 loss 0.2331265070941299\n",
      "step 29820 loss 0.25963496742770076\n",
      "step 29840 loss 0.24211029075086116\n",
      "step 29860 loss 0.27388007324188945\n",
      "step 29880 loss 0.26603661795379596\n",
      "step 29900 loss 0.1773665409302339\n",
      "step 29920 loss 0.2597303347662091\n",
      "step 29940 loss 0.20520269158296287\n",
      "step 29960 loss 0.1975743578746915\n",
      "step 29980 loss 0.2229504463262856\n",
      "step 30000 loss 0.19134017322212457\n",
      "step 30020 loss 0.24385068183764816\n",
      "step 30040 loss 0.2100019625853747\n",
      "step 30060 loss 0.16304900632239877\n",
      "step 30080 loss 0.1844259075820446\n",
      "step 30100 loss 0.19573469096794724\n",
      "step 30120 loss 0.12960424541379326\n",
      "step 30140 loss 0.1853411871008575\n",
      "step 30160 loss 0.21952622504904867\n",
      "step 30180 loss 0.24534808993339538\n",
      "step 30200 loss 0.24545610649511218\n",
      "step 30220 loss 0.1907046243548393\n",
      "step 30240 loss 0.246002180268988\n",
      "step 30260 loss 0.21109029692597686\n",
      "step 30280 loss 0.29106352927628903\n",
      "step 30300 loss 0.21423343410715462\n",
      "step 30320 loss 0.2398358445148915\n",
      "step 30340 loss 0.16562421498820185\n",
      "step 30360 loss 0.17520351633429526\n",
      "step 30380 loss 0.25872517125681044\n",
      "step 30400 loss 0.19320257054641843\n",
      "step 30420 loss 0.1614101282786578\n",
      "step 30440 loss 0.16778984492411836\n",
      "step 30460 loss 0.19885634807869793\n",
      "step 30480 loss 0.16829501939937472\n",
      "step 30500 loss 0.19198501203209162\n",
      "step 30520 loss 0.2247697661165148\n",
      "step 30540 loss 0.2489625273272395\n",
      "step 30560 loss 0.2675528137944639\n",
      "step 30580 loss 0.1635128617985174\n",
      "step 30600 loss 0.191981481295079\n",
      "step 30620 loss 0.23227652655914427\n",
      "step 30640 loss 0.2186698205769062\n",
      "step 30660 loss 0.24201175849884748\n",
      "step 30680 loss 0.2569379135966301\n",
      "step 30700 loss 0.26750028841197493\n",
      "step 30720 loss 0.24448256455361844\n",
      "step 30740 loss 0.2526280826423317\n",
      "step 30760 loss 0.20423866799101234\n",
      "step 30780 loss 0.19725383543409408\n",
      "step 30800 loss 0.2083477851934731\n",
      "step 30820 loss 0.20574627434834838\n",
      "step 30840 loss 0.1764257334638387\n",
      "step 30860 loss 0.18369702803902327\n",
      "step 30880 loss 0.1909863361157477\n",
      "step 30900 loss 0.2459564162651077\n",
      "step 30920 loss 0.17989225750789045\n",
      "step 30940 loss 0.193902490939945\n",
      "step 30960 loss 0.2780529054813087\n",
      "step 30980 loss 0.20626515736803414\n",
      "step 31000 loss 0.18473316630115733\n",
      "step 31020 loss 0.16822890508919955\n",
      "step 31040 loss 0.16464742850512265\n",
      "step 31060 loss 0.24858205202035605\n",
      "step 31080 loss 0.25340339466929435\n",
      "step 31100 loss 0.14581990116275848\n",
      "step 31120 loss 0.20755771175026894\n",
      "step 31140 loss 0.21698525901883842\n",
      "step 31160 loss 0.26818424919620154\n",
      "step 31180 loss 0.2290159140713513\n",
      "step 31200 loss 0.14251636835979298\n",
      "step 31220 loss 0.18629200078430586\n",
      "step 31240 loss 0.2800667815841734\n",
      "step 31260 loss 0.15027214610017836\n",
      "step 31280 loss 0.20510769188404082\n",
      "step 31300 loss 0.17377403448335826\n",
      "step 31320 loss 0.19493062286637725\n",
      "step 31340 loss 0.2448028356768191\n",
      "step 31360 loss 0.20133600707631558\n",
      "step 31380 loss 0.15667272778227925\n",
      "step 31400 loss 0.17646168707869947\n",
      "step 31420 loss 0.20812585204839706\n",
      "step 31440 loss 0.22552580633200706\n",
      "step 31460 loss 0.23875077052507548\n",
      "step 31480 loss 0.2248631961643696\n",
      "step 31500 loss 0.15175159324426205\n",
      "step 31520 loss 0.18230917826294898\n",
      "step 31540 loss 0.2263759566936642\n",
      "step 31560 loss 0.21833845172077418\n",
      "step 31580 loss 0.10374663891270756\n",
      "step 31600 loss 0.22933913310989737\n",
      "step 31620 loss 0.21799962953664362\n",
      "step 31640 loss 0.2222361968830228\n",
      "step 31660 loss 0.21067738328129054\n",
      "step 31680 loss 0.23315216354094445\n",
      "step 31700 loss 0.18359300410374998\n",
      "step 31720 loss 0.16609902447089553\n",
      "step 31740 loss 0.1992664569290355\n",
      "step 31760 loss 0.15048618223518134\n",
      "step 31780 loss 0.24696026076562702\n",
      "step 31800 loss 0.25272285044193266\n",
      "step 31820 loss 0.20911523094400764\n",
      "step 31840 loss 0.20338438618928195\n",
      "step 31860 loss 0.16803333703428508\n",
      "step 31880 loss 0.22412989945150913\n",
      "step 31900 loss 0.19153060978860595\n",
      "step 31920 loss 0.21498546047951095\n",
      "step 31940 loss 0.29036062883678826\n",
      "step 31960 loss 0.15492858353536576\n",
      "step 31980 loss 0.19622259465977548\n",
      "step 32000 loss 0.1865103732328862\n",
      "step 32020 loss 0.26401149202138186\n",
      "step 32040 loss 0.23463623551651835\n",
      "step 32060 loss 0.18805559785105289\n",
      "step 32080 loss 0.2646703099599108\n",
      "step 32100 loss 0.2700765259563923\n",
      "step 32120 loss 0.18115433249622584\n",
      "step 32140 loss 0.1769716280978173\n",
      "step 32160 loss 0.19259879251476378\n",
      "step 32180 loss 0.2439585420070216\n",
      "step 32200 loss 0.1758023465750739\n",
      "step 32220 loss 0.21966027384623885\n",
      "step 32240 loss 0.11216798417735845\n",
      "step 32260 loss 0.2377991575282067\n",
      "step 32280 loss 0.1740190338343382\n",
      "step 32300 loss 0.189036221569404\n",
      "step 32320 loss 0.15619080429896712\n",
      "step 32340 loss 0.23902663692133502\n",
      "step 32360 loss 0.1282212414778769\n",
      "step 32380 loss 0.2622220518998802\n",
      "step 32400 loss 0.18104784802999346\n",
      "step 32420 loss 0.17485436035203747\n",
      "step 32440 loss 0.15788550497964024\n",
      "step 32460 loss 0.2139572821557522\n",
      "step 32480 loss 0.13575868639163674\n",
      "step 32500 loss 0.207119061332196\n",
      "step 32520 loss 0.12728115282952784\n",
      "step 32540 loss 0.1476013220846653\n",
      "step 32560 loss 0.19047464290633798\n",
      "step 32580 loss 0.1673669687472284\n",
      "step 32600 loss 0.18305339189246297\n",
      "step 32620 loss 0.13533461991464718\n",
      "step 32640 loss 0.19427934559062124\n",
      "step 32660 loss 0.1411266434006393\n",
      "step 32680 loss 0.21970970733091236\n",
      "step 32700 loss 0.19543665426317602\n",
      "step 32720 loss 0.17760734735056757\n",
      "step 32740 loss 0.16613735868595542\n",
      "step 32760 loss 0.17823757442529314\n",
      "step 32780 loss 0.2417764597106725\n",
      "step 32800 loss 0.17704575366806238\n",
      "step 32820 loss 0.19492283249273895\n",
      "step 32840 loss 0.19328319067135452\n",
      "step 32860 loss 0.24599141161888838\n",
      "step 32880 loss 0.16969266417436302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 32900 loss 0.22904159016907216\n",
      "step 32920 loss 0.2712306783068925\n",
      "step 32940 loss 0.17714457735419273\n",
      "step 32960 loss 0.19144387478008867\n",
      "step 32980 loss 0.14511584541760386\n",
      "step 33000 loss 0.2119609140790999\n",
      "step 33020 loss 0.20782706884201615\n",
      "step 33040 loss 0.17741785626858472\n",
      "step 33060 loss 0.15858169852290302\n",
      "step 33080 loss 0.1161962991231121\n",
      "step 33100 loss 0.14787431857548655\n",
      "step 33120 loss 0.2580953758209944\n",
      "step 33140 loss 0.13205309328623116\n",
      "step 33160 loss 0.2455327871721238\n",
      "step 33180 loss 0.19613259020261467\n",
      "step 33200 loss 0.20973005653358995\n",
      "step 33220 loss 0.1819356913678348\n",
      "step 33240 loss 0.2119174921186641\n",
      "step 33260 loss 0.20861450335942208\n",
      "step 33280 loss 0.13584714690223337\n",
      "step 33300 loss 0.17257836415665223\n",
      "step 33320 loss 0.28262497978284956\n",
      "step 33340 loss 0.29599538519978524\n",
      "step 33360 loss 0.20067960342857988\n",
      "step 33380 loss 0.18803939083591104\n",
      "step 33400 loss 0.16285536924842745\n",
      "step 33420 loss 0.13627954265102743\n",
      "step 33440 loss 0.17981208092533052\n",
      "step 33460 loss 0.2655012091621757\n",
      "step 33480 loss 0.24418161485809833\n",
      "step 33500 loss 0.21539570349268616\n",
      "step 33520 loss 0.20473634637892246\n",
      "step 33540 loss 0.22234293892979623\n",
      "step 33560 loss 0.13206148352473973\n",
      "step 33580 loss 0.19132545616012067\n",
      "step 33600 loss 0.22763886777684092\n",
      "step 33620 loss 0.21240525403991342\n",
      "step 33640 loss 0.2304073770530522\n",
      "step 33660 loss 0.17248468108009546\n",
      "step 33680 loss 0.21277052930090576\n",
      "step 33700 loss 0.1803381853271276\n",
      "step 33720 loss 0.19262141971848906\n",
      "step 33740 loss 0.18300386248156428\n",
      "step 33760 loss 0.21828421475365758\n",
      "step 33780 loss 0.2813333799364045\n",
      "step 33800 loss 0.1913050301373005\n",
      "step 33820 loss 0.1955186511389911\n",
      "step 33840 loss 0.23751762472093105\n",
      "step 33860 loss 0.1960615122690797\n",
      "step 33880 loss 0.2113520560786128\n",
      "step 33900 loss 0.30091341473162175\n",
      "step 33920 loss 0.15931398877874017\n",
      "step 33940 loss 0.17350495707942173\n",
      "step 33960 loss 0.2442495109513402\n",
      "step 33980 loss 0.1491049136966467\n",
      "step 34000 loss 0.22953757639043032\n",
      "step 34020 loss 0.160673146834597\n",
      "step 34040 loss 0.141593969182577\n",
      "step 34060 loss 0.2257757168263197\n",
      "step 34080 loss 0.24662524443119765\n",
      "step 34100 loss 0.2676034505479038\n",
      "step 34120 loss 0.18901305231265725\n",
      "step 34140 loss 0.13418907201848923\n",
      "step 34160 loss 0.17415749914944173\n",
      "step 34180 loss 0.17993011584039778\n",
      "step 34200 loss 0.201065378729254\n",
      "step 34220 loss 0.15000364320585505\n",
      "step 34240 loss 0.18409925252199172\n",
      "step 34260 loss 0.1961887245066464\n",
      "step 34280 loss 0.17111153528094292\n",
      "EPOCH 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c588b0161044f2803c5762ed747062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.21089195092208685\n",
      "step 20 loss 0.2376395340077579\n",
      "step 40 loss 0.22385706841014325\n",
      "step 60 loss 0.1521665283245966\n",
      "step 80 loss 0.214618634339422\n",
      "step 100 loss 0.10192731637507677\n",
      "step 120 loss 0.2147181489272043\n",
      "step 140 loss 0.15944940337794833\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-660adfb311cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# печатаем скользящее среднее значение функции потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreport_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreport_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 16  # сколько примеров показываем модели за один шаг\n",
    "report_steps = 20  # раз в сколько шагов печатаем результат\n",
    "epochs = 10  # сколько раз мы покажем данные модели\n",
    "model.cuda()\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH', epoch)\n",
    "    random.shuffle(pairs)\n",
    "    for i in trange(0, int(len(pairs) / batch_size)):\n",
    "        batch = pairs[i * batch_size: (i + 1) * batch_size]\n",
    "        # кодируем вопрос и ответ \n",
    "        x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "        y = tokenizer([p[1] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "        # -100 - специальное значение, позволяющее не учитывать токены\n",
    "        y.input_ids[y.input_ids == 0] = -100\n",
    "        # вычисляем функцию потерь\n",
    "        loss = model(\n",
    "            input_ids=x.input_ids,\n",
    "            attention_mask=x.attention_mask,\n",
    "            labels=y.input_ids,\n",
    "            decoder_attention_mask=y.attention_mask,\n",
    "            return_dict=True\n",
    "        ).loss\n",
    "        # делаем шаг градиентного спуска\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # печатаем скользящее среднее значение функции потерь\n",
    "        losses.append(loss.item())\n",
    "        if i % report_steps == 0:\n",
    "            print('step', i, 'loss', np.mean(losses[-report_steps:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../data/t5_trainded.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'западный штат'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Нынешний ___ президент Джо Байден заявил, что вопрос пошлин на импорт китайских товаров в США находится на рассмотрении, и он намерен обсудить его с министром финансов страны Джанет Йеллен после своего возвращения из азиатского турне, сообщило агентство Bloomberg. Эти слова Байдена были интерпретированы инвесторами как сигнал возможной отмены некоторых пошлин.\"\n",
    "generate(model, f\" fill сша | {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbr</th>\n",
       "      <th>desc</th>\n",
       "      <th>desc_norm</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>abbr_norm</th>\n",
       "      <th>abbr_len</th>\n",
       "      <th>abbr_count</th>\n",
       "      <th>desc_count</th>\n",
       "      <th>abbr_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>каэр</td>\n",
       "      <td>КР</td>\n",
       "      <td>кр</td>\n",
       "      <td>1</td>\n",
       "      <td>каэр</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>133344</td>\n",
       "      <td>1647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>гг.</td>\n",
       "      <td>годы</td>\n",
       "      <td>год</td>\n",
       "      <td>1</td>\n",
       "      <td>год</td>\n",
       "      <td>1</td>\n",
       "      <td>114333</td>\n",
       "      <td>114333</td>\n",
       "      <td>2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>кот.</td>\n",
       "      <td>который</td>\n",
       "      <td>который</td>\n",
       "      <td>1</td>\n",
       "      <td>кот</td>\n",
       "      <td>1</td>\n",
       "      <td>81160</td>\n",
       "      <td>76083</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>нт</td>\n",
       "      <td>нит</td>\n",
       "      <td>нит</td>\n",
       "      <td>1</td>\n",
       "      <td>нт</td>\n",
       "      <td>1</td>\n",
       "      <td>215231</td>\n",
       "      <td>58791</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>дн</td>\n",
       "      <td>дина</td>\n",
       "      <td>дин</td>\n",
       "      <td>1</td>\n",
       "      <td>дн</td>\n",
       "      <td>1</td>\n",
       "      <td>104033</td>\n",
       "      <td>53587</td>\n",
       "      <td>4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>Рос.</td>\n",
       "      <td>Россия</td>\n",
       "      <td>россия</td>\n",
       "      <td>1</td>\n",
       "      <td>расти</td>\n",
       "      <td>1</td>\n",
       "      <td>4549</td>\n",
       "      <td>44827</td>\n",
       "      <td>2327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>тж.</td>\n",
       "      <td>также;</td>\n",
       "      <td>также</td>\n",
       "      <td>1</td>\n",
       "      <td>тж</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>33827</td>\n",
       "      <td>5102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>вв.</td>\n",
       "      <td>века</td>\n",
       "      <td>век</td>\n",
       "      <td>1</td>\n",
       "      <td>век</td>\n",
       "      <td>1</td>\n",
       "      <td>29348</td>\n",
       "      <td>29348</td>\n",
       "      <td>3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>вр.</td>\n",
       "      <td>время</td>\n",
       "      <td>время</td>\n",
       "      <td>1</td>\n",
       "      <td>вр</td>\n",
       "      <td>1</td>\n",
       "      <td>76411</td>\n",
       "      <td>28925</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>чел.</td>\n",
       "      <td>человек</td>\n",
       "      <td>человек</td>\n",
       "      <td>1</td>\n",
       "      <td>чел</td>\n",
       "      <td>1</td>\n",
       "      <td>30758</td>\n",
       "      <td>27581</td>\n",
       "      <td>4447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>заявл.</td>\n",
       "      <td>заявлено;</td>\n",
       "      <td>заявить</td>\n",
       "      <td>1</td>\n",
       "      <td>заявл</td>\n",
       "      <td>1</td>\n",
       "      <td>8527</td>\n",
       "      <td>25559</td>\n",
       "      <td>3293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>раб.</td>\n",
       "      <td>работа</td>\n",
       "      <td>работа</td>\n",
       "      <td>1</td>\n",
       "      <td>раб</td>\n",
       "      <td>1</td>\n",
       "      <td>42837</td>\n",
       "      <td>24386</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>рос.</td>\n",
       "      <td>Российский</td>\n",
       "      <td>российский</td>\n",
       "      <td>1</td>\n",
       "      <td>расти</td>\n",
       "      <td>1</td>\n",
       "      <td>4549</td>\n",
       "      <td>24384</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>росс.</td>\n",
       "      <td>Российский</td>\n",
       "      <td>российский</td>\n",
       "      <td>1</td>\n",
       "      <td>росс</td>\n",
       "      <td>1</td>\n",
       "      <td>71393</td>\n",
       "      <td>24384</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>д.</td>\n",
       "      <td>дом</td>\n",
       "      <td>дом</td>\n",
       "      <td>1</td>\n",
       "      <td>далее</td>\n",
       "      <td>1</td>\n",
       "      <td>641</td>\n",
       "      <td>24025</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>през.</td>\n",
       "      <td>президент</td>\n",
       "      <td>президент</td>\n",
       "      <td>1</td>\n",
       "      <td>през</td>\n",
       "      <td>1</td>\n",
       "      <td>24579</td>\n",
       "      <td>23705</td>\n",
       "      <td>3203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>проц.</td>\n",
       "      <td>процент</td>\n",
       "      <td>процент</td>\n",
       "      <td>1</td>\n",
       "      <td>проц</td>\n",
       "      <td>1</td>\n",
       "      <td>28697</td>\n",
       "      <td>23701</td>\n",
       "      <td>3665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>гол.</td>\n",
       "      <td>голов</td>\n",
       "      <td>гол</td>\n",
       "      <td>1</td>\n",
       "      <td>гол</td>\n",
       "      <td>1</td>\n",
       "      <td>23012</td>\n",
       "      <td>23012</td>\n",
       "      <td>5136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>сл.</td>\n",
       "      <td>слово</td>\n",
       "      <td>слово</td>\n",
       "      <td>1</td>\n",
       "      <td>сл</td>\n",
       "      <td>1</td>\n",
       "      <td>180189</td>\n",
       "      <td>22658</td>\n",
       "      <td>2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>нов.</td>\n",
       "      <td>Новый</td>\n",
       "      <td>новый</td>\n",
       "      <td>1</td>\n",
       "      <td>новый</td>\n",
       "      <td>1</td>\n",
       "      <td>21437</td>\n",
       "      <td>21437</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>тыс.</td>\n",
       "      <td>тысяча</td>\n",
       "      <td>тысяча</td>\n",
       "      <td>1</td>\n",
       "      <td>тыс</td>\n",
       "      <td>1</td>\n",
       "      <td>20358</td>\n",
       "      <td>19979</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>гор.</td>\n",
       "      <td>город</td>\n",
       "      <td>город</td>\n",
       "      <td>1</td>\n",
       "      <td>гора</td>\n",
       "      <td>1</td>\n",
       "      <td>2616</td>\n",
       "      <td>19689</td>\n",
       "      <td>4307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4935</th>\n",
       "      <td>гг.</td>\n",
       "      <td>города</td>\n",
       "      <td>город</td>\n",
       "      <td>1</td>\n",
       "      <td>год</td>\n",
       "      <td>1</td>\n",
       "      <td>114333</td>\n",
       "      <td>19689</td>\n",
       "      <td>4935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>дол.</td>\n",
       "      <td>доллар</td>\n",
       "      <td>доллар</td>\n",
       "      <td>1</td>\n",
       "      <td>дол</td>\n",
       "      <td>1</td>\n",
       "      <td>51669</td>\n",
       "      <td>19420</td>\n",
       "      <td>3547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>долл.</td>\n",
       "      <td>доллар</td>\n",
       "      <td>доллар</td>\n",
       "      <td>1</td>\n",
       "      <td>долл</td>\n",
       "      <td>1</td>\n",
       "      <td>19446</td>\n",
       "      <td>19420</td>\n",
       "      <td>3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>млн</td>\n",
       "      <td>миллион</td>\n",
       "      <td>миллион</td>\n",
       "      <td>1</td>\n",
       "      <td>млн</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>18889</td>\n",
       "      <td>4983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>здн</td>\n",
       "      <td>здание</td>\n",
       "      <td>здание</td>\n",
       "      <td>1</td>\n",
       "      <td>здн</td>\n",
       "      <td>1</td>\n",
       "      <td>7198</td>\n",
       "      <td>18769</td>\n",
       "      <td>4331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4330</th>\n",
       "      <td>зд.</td>\n",
       "      <td>здание</td>\n",
       "      <td>здание</td>\n",
       "      <td>1</td>\n",
       "      <td>зд</td>\n",
       "      <td>1</td>\n",
       "      <td>59011</td>\n",
       "      <td>18769</td>\n",
       "      <td>4330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>мин.</td>\n",
       "      <td>министр</td>\n",
       "      <td>министр</td>\n",
       "      <td>1</td>\n",
       "      <td>мина</td>\n",
       "      <td>1</td>\n",
       "      <td>5237</td>\n",
       "      <td>18356</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>дн.</td>\n",
       "      <td>дни</td>\n",
       "      <td>день</td>\n",
       "      <td>1</td>\n",
       "      <td>дн</td>\n",
       "      <td>1</td>\n",
       "      <td>104033</td>\n",
       "      <td>17838</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2302</th>\n",
       "      <td>д.</td>\n",
       "      <td>день</td>\n",
       "      <td>день</td>\n",
       "      <td>1</td>\n",
       "      <td>далее</td>\n",
       "      <td>1</td>\n",
       "      <td>641</td>\n",
       "      <td>17838</td>\n",
       "      <td>2302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>тт.</td>\n",
       "      <td>тома</td>\n",
       "      <td>том</td>\n",
       "      <td>1</td>\n",
       "      <td>тт</td>\n",
       "      <td>1</td>\n",
       "      <td>7742</td>\n",
       "      <td>16996</td>\n",
       "      <td>4345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>дд.</td>\n",
       "      <td>дела</td>\n",
       "      <td>дело</td>\n",
       "      <td>1</td>\n",
       "      <td>дд</td>\n",
       "      <td>1</td>\n",
       "      <td>10945</td>\n",
       "      <td>16930</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>Д.</td>\n",
       "      <td>дело</td>\n",
       "      <td>дело</td>\n",
       "      <td>1</td>\n",
       "      <td>далее</td>\n",
       "      <td>1</td>\n",
       "      <td>641</td>\n",
       "      <td>16930</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>ер.</td>\n",
       "      <td>ерик</td>\n",
       "      <td>ерик</td>\n",
       "      <td>1</td>\n",
       "      <td>ер</td>\n",
       "      <td>1</td>\n",
       "      <td>548001</td>\n",
       "      <td>16907</td>\n",
       "      <td>1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>тр.</td>\n",
       "      <td>труд;</td>\n",
       "      <td>труд</td>\n",
       "      <td>1</td>\n",
       "      <td>тр</td>\n",
       "      <td>1</td>\n",
       "      <td>315334</td>\n",
       "      <td>16718</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>кл.</td>\n",
       "      <td>ключ</td>\n",
       "      <td>ключ</td>\n",
       "      <td>1</td>\n",
       "      <td>кл</td>\n",
       "      <td>1</td>\n",
       "      <td>51386</td>\n",
       "      <td>16214</td>\n",
       "      <td>1152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4959</th>\n",
       "      <td>предст.</td>\n",
       "      <td>представитель</td>\n",
       "      <td>представитель</td>\n",
       "      <td>1</td>\n",
       "      <td>предст</td>\n",
       "      <td>1</td>\n",
       "      <td>25077</td>\n",
       "      <td>16079</td>\n",
       "      <td>4959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>мск</td>\n",
       "      <td>Москва;</td>\n",
       "      <td>москва</td>\n",
       "      <td>1</td>\n",
       "      <td>мск</td>\n",
       "      <td>1</td>\n",
       "      <td>5839</td>\n",
       "      <td>15977</td>\n",
       "      <td>3086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>бр.</td>\n",
       "      <td>братья</td>\n",
       "      <td>брат</td>\n",
       "      <td>1</td>\n",
       "      <td>бр</td>\n",
       "      <td>1</td>\n",
       "      <td>111039</td>\n",
       "      <td>15831</td>\n",
       "      <td>4627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>лл.</td>\n",
       "      <td>листы</td>\n",
       "      <td>лист</td>\n",
       "      <td>1</td>\n",
       "      <td>лл</td>\n",
       "      <td>1</td>\n",
       "      <td>78628</td>\n",
       "      <td>15788</td>\n",
       "      <td>2345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>руб.</td>\n",
       "      <td>рубль</td>\n",
       "      <td>рубль</td>\n",
       "      <td>1</td>\n",
       "      <td>руб</td>\n",
       "      <td>1</td>\n",
       "      <td>19298</td>\n",
       "      <td>14789</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>гл.</td>\n",
       "      <td>глава</td>\n",
       "      <td>глава</td>\n",
       "      <td>1</td>\n",
       "      <td>гл</td>\n",
       "      <td>1</td>\n",
       "      <td>56837</td>\n",
       "      <td>14746</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2297</th>\n",
       "      <td>др.</td>\n",
       "      <td>другой</td>\n",
       "      <td>другой</td>\n",
       "      <td>1</td>\n",
       "      <td>др</td>\n",
       "      <td>1</td>\n",
       "      <td>55229</td>\n",
       "      <td>14051</td>\n",
       "      <td>2297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>ЩЛ</td>\n",
       "      <td>ОК</td>\n",
       "      <td>около</td>\n",
       "      <td>1</td>\n",
       "      <td>щл</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13958</td>\n",
       "      <td>3873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ок.</td>\n",
       "      <td>около</td>\n",
       "      <td>около</td>\n",
       "      <td>1</td>\n",
       "      <td>около</td>\n",
       "      <td>1</td>\n",
       "      <td>13958</td>\n",
       "      <td>13958</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>сп.</td>\n",
       "      <td>спорт</td>\n",
       "      <td>спорт</td>\n",
       "      <td>1</td>\n",
       "      <td>сп</td>\n",
       "      <td>1</td>\n",
       "      <td>133074</td>\n",
       "      <td>13651</td>\n",
       "      <td>3335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>дк</td>\n",
       "      <td>дека...</td>\n",
       "      <td>дек</td>\n",
       "      <td>1</td>\n",
       "      <td>дк</td>\n",
       "      <td>1</td>\n",
       "      <td>12419</td>\n",
       "      <td>12934</td>\n",
       "      <td>4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>пр.</td>\n",
       "      <td>проект</td>\n",
       "      <td>проект</td>\n",
       "      <td>1</td>\n",
       "      <td>пр</td>\n",
       "      <td>1</td>\n",
       "      <td>639983</td>\n",
       "      <td>12791</td>\n",
       "      <td>2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>ам.</td>\n",
       "      <td>американский</td>\n",
       "      <td>американский</td>\n",
       "      <td>1</td>\n",
       "      <td>ама</td>\n",
       "      <td>1</td>\n",
       "      <td>9641</td>\n",
       "      <td>12696</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         abbr           desc      desc_norm  desc_len abbr_norm  abbr_len  \\\n",
       "1647     каэр             КР             кр         1      каэр         1   \n",
       "2568      гг.           годы            год         1       год         1   \n",
       "1503     кот.        который        который         1       кот         1   \n",
       "1701       нт            нит            нит         1        нт         1   \n",
       "4085       дн           дина            дин         1        дн         1   \n",
       "2327     Рос.         Россия         россия         1     расти         1   \n",
       "5102      тж.         также;          также         1        тж         1   \n",
       "3152      вв.           века            век         1       век         1   \n",
       "461       вр.          время          время         1        вр         1   \n",
       "4447     чел.        человек        человек         1       чел         1   \n",
       "3293   заявл.      заявлено;        заявить         1     заявл         1   \n",
       "1226     раб.         работа         работа         1       раб         1   \n",
       "65       рос.     Российский     российский         1     расти         1   \n",
       "66      росс.     Российский     российский         1      росс         1   \n",
       "3249       д.            дом            дом         1     далее         1   \n",
       "3203    през.      президент      президент         1      през         1   \n",
       "3665    проц.        процент        процент         1      проц         1   \n",
       "5136     гол.          голов            гол         1       гол         1   \n",
       "2498      сл.          слово          слово         1        сл         1   \n",
       "1978     нов.          Новый          новый         1     новый         1   \n",
       "1341     тыс.         тысяча         тысяча         1       тыс         1   \n",
       "4307     гор.          город          город         1      гора         1   \n",
       "4935      гг.         города          город         1       год         1   \n",
       "3547     дол.         доллар         доллар         1       дол         1   \n",
       "3548    долл.         доллар         доллар         1      долл         1   \n",
       "4983      млн        миллион        миллион         1       млн         1   \n",
       "4331      здн         здание         здание         1       здн         1   \n",
       "4330      зд.         здание         здание         1        зд         1   \n",
       "699      мин.        министр        министр         1      мина         1   \n",
       "376       дн.            дни           день         1        дн         1   \n",
       "2302       д.           день           день         1     далее         1   \n",
       "4345      тт.           тома            том         1        тт         1   \n",
       "1055      дд.           дела           дело         1        дд         1   \n",
       "1231       Д.           дело           дело         1     далее         1   \n",
       "1846      ер.           ерик           ерик         1        ер         1   \n",
       "326       тр.          труд;           труд         1        тр         1   \n",
       "1152      кл.           ключ           ключ         1        кл         1   \n",
       "4959  предст.  представитель  представитель         1    предст         1   \n",
       "3086      мск        Москва;         москва         1       мск         1   \n",
       "4627      бр.         братья           брат         1        бр         1   \n",
       "2345      лл.          листы           лист         1        лл         1   \n",
       "85       руб.          рубль          рубль         1       руб         1   \n",
       "390       гл.          глава          глава         1        гл         1   \n",
       "2297      др.         другой         другой         1        др         1   \n",
       "3873       ЩЛ             ОК          около         1        щл         1   \n",
       "997       ок.          около          около         1     около         1   \n",
       "3335      сп.          спорт          спорт         1        сп         1   \n",
       "4019       дк        дека...            дек         1        дк         1   \n",
       "2636      пр.         проект         проект         1        пр         1   \n",
       "1119      ам.   американский   американский         1       ама         1   \n",
       "\n",
       "      abbr_count  desc_count  abbr_id  \n",
       "1647           0      133344     1647  \n",
       "2568      114333      114333     2568  \n",
       "1503       81160       76083     1503  \n",
       "1701      215231       58791     1701  \n",
       "4085      104033       53587     4085  \n",
       "2327        4549       44827     2327  \n",
       "5102          43       33827     5102  \n",
       "3152       29348       29348     3152  \n",
       "461        76411       28925      461  \n",
       "4447       30758       27581     4447  \n",
       "3293        8527       25559     3293  \n",
       "1226       42837       24386     1226  \n",
       "65          4549       24384       65  \n",
       "66         71393       24384       66  \n",
       "3249         641       24025     3249  \n",
       "3203       24579       23705     3203  \n",
       "3665       28697       23701     3665  \n",
       "5136       23012       23012     5136  \n",
       "2498      180189       22658     2498  \n",
       "1978       21437       21437     1978  \n",
       "1341       20358       19979     1341  \n",
       "4307        2616       19689     4307  \n",
       "4935      114333       19689     4935  \n",
       "3547       51669       19420     3547  \n",
       "3548       19446       19420     3548  \n",
       "4983          24       18889     4983  \n",
       "4331        7198       18769     4331  \n",
       "4330       59011       18769     4330  \n",
       "699         5237       18356      699  \n",
       "376       104033       17838      376  \n",
       "2302         641       17838     2302  \n",
       "4345        7742       16996     4345  \n",
       "1055       10945       16930     1055  \n",
       "1231         641       16930     1231  \n",
       "1846      548001       16907     1846  \n",
       "326       315334       16718      326  \n",
       "1152       51386       16214     1152  \n",
       "4959       25077       16079     4959  \n",
       "3086        5839       15977     3086  \n",
       "4627      111039       15831     4627  \n",
       "2345       78628       15788     2345  \n",
       "85         19298       14789       85  \n",
       "390        56837       14746      390  \n",
       "2297       55229       14051     2297  \n",
       "3873           1       13958     3873  \n",
       "997        13958       13958      997  \n",
       "3335      133074       13651     3335  \n",
       "4019       12419       12934     4019  \n",
       "2636      639983       12791     2636  \n",
       "1119        9641       12696     1119  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abbr.sort_values(\"desc_count\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
